{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4931,"status":"ok","timestamp":1651725437761,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"},"user_tz":240},"id":"xMYMvgbZDClz","outputId":"f739a77b-2f5d-488d-ebc5-64b730c5447b"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 2.7 ms (started: 2022-05-05 04:37:15 +00:00)\n"]}],"source":["%%capture\n","!pip install ipython-autotime\n","\n","%load_ext autotime"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XuTgLgoSku-g","outputId":"26931ab6-f367-4088-b702-709d72d2bd8d","executionInfo":{"status":"ok","timestamp":1651725459576,"user_tz":240,"elapsed":21826,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","time: 22.3 s (started: 2022-05-05 04:37:15 +00:00)\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0PsH3UnqkzID","outputId":"08052165-88fa-4d09-f0aa-46f83b380315","executionInfo":{"status":"ok","timestamp":1651725464171,"user_tz":240,"elapsed":4612,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 4.96 s (started: 2022-05-05 04:37:37 +00:00)\n"]}],"source":["!cp -r '/content/gdrive/MyDrive/ESE_650_Project/data/' '/content/'"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wh0CU0RLl4ot","outputId":"1eab3329-7923-4b92-d85c-fd4aab1d8ec8","executionInfo":{"status":"ok","timestamp":1651725465015,"user_tz":240,"elapsed":848,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["rm: cannot remove '/content/Data': No such file or directory\n","time: 121 ms (started: 2022-05-05 04:37:42 +00:00)\n"]}],"source":["!rm -r '/content/Data'"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C4O7BDpgk9wv","outputId":"7852cac5-bd1d-4635-e531-d7712600f2d1","executionInfo":{"status":"ok","timestamp":1651725468376,"user_tz":240,"elapsed":3368,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 3.9 s (started: 2022-05-05 04:37:42 +00:00)\n"]}],"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","\n","from tqdm import tqdm\n","\n","import matplotlib.pyplot as plt\n","\n","\n","## scene loader \n","import sys\n","import h5py\n","import json\n","import numpy as np\n","import random\n","import skimage.io\n","from skimage.transform import resize\n","\n","import os"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SSArD4e_zjRt","outputId":"8a483414-997e-4094-ba6b-43585d0b8636","executionInfo":{"status":"ok","timestamp":1651725468377,"user_tz":240,"elapsed":16,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 19.3 ms (started: 2022-05-05 04:37:46 +00:00)\n"]}],"source":["def seed_everything(seed=42):\n","  random.seed(seed)\n","  os.environ['PYTHONHASHSEED'] = str(seed)\n","  np.random.seed(seed)\n","  torch.manual_seed(seed)\n","  torch.backends.cudnn.deterministic = True\n","  torch.backends.cudnn.benchmark = False\n","  np.random.seed(seed)\n","\n","  return\n","\n","seed_everything()"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"c8MS6DQBmvGW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651725468378,"user_tz":240,"elapsed":11,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}},"outputId":"99776642-6e3d-4916-d862-5b7053606ea8"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 6.4 ms (started: 2022-05-05 04:37:46 +00:00)\n"]}],"source":["ACTION_SIZE = 4 # action size\n","\n","SCREEN_WIDTH = 84\n","SCREEN_HEIGHT = 84\n","HISTORY_LENGTH = 4\n","\n","NUM_EVAL_EPISODES = 5\n","\n","TASK_TYPE = 'navigation' # no need to change\n","# keys are scene names, and values are a list of location ids (navigation targets)\n","TASK_LIST = {\n","  'bathroom_02'    : ['26', '37', '43', '53', '69'],\n","  'bedroom_04'     : ['134', '264', '320', '384', '387'],\n","  'kitchen_02'     : ['90', '136', '157', '207', '329'],\n","  'living_room_08' : ['92', '135', '193', '228', '254']\n","}"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":420,"status":"ok","timestamp":1651725468791,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"},"user_tz":240},"id":"J3a0P56lmyuo","outputId":"3351bebe-7387-435c-9624-1b7ec98b8b20"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 315 ms (started: 2022-05-05 04:37:46 +00:00)\n"]}],"source":["## Scene Loader\n","\n","# -*- coding: utf-8 -*-\n","class THORDiscreteEnvironment(object):\n","\n","  def __init__(self, config=dict()):\n","\n","    # configurations\n","    self.scene_name          = config.get('scene_name', 'bedroom_04')\n","    self.random_start        = config.get('random_start', True)\n","    self.n_feat_per_locaiton = config.get('n_feat_per_locaiton', 1) # 1 for no sampling\n","    self.terminal_state_id   = config.get('terminal_state_id', 0)\n","\n","    self.h5_file_path = config.get('h5_file_path', 'data/%s.h5'%self.scene_name)\n","    self.h5_file      = h5py.File(self.h5_file_path, 'r')\n","\n","    self.locations   = self.h5_file['location'][()]\n","    self.rotations   = self.h5_file['rotation'][()]\n","    self.n_locations = self.locations.shape[0]\n","\n","    self.terminals = np.zeros(self.n_locations)\n","    self.terminals[self.terminal_state_id] = 1\n","    self.terminal_states, = np.where(self.terminals)\n","\n","    self.transition_graph = self.h5_file['graph'][()]\n","    self.shortest_path_distances = self.h5_file['shortest_path_distance'][()]\n","\n","    self.history_length = HISTORY_LENGTH\n","    self.screen_height  = SCREEN_HEIGHT\n","    self.screen_width   = SCREEN_WIDTH\n","\n","    # we use pre-computed fc7 features from ResNet-50\n","    # self.s_t = np.zeros([self.screen_height, self.screen_width, self.history_length])\n","    self.s_t      = np.zeros([2048, self.history_length])\n","    self.s_t1     = np.zeros_like(self.s_t)\n","    self.s_target = self._tiled_state(self.terminal_state_id)\n","\n","    self.reset()\n","\n","  # public methods\n","\n","  def reset(self):\n","    # randomize initial state\n","    while True:\n","      k = random.randrange(self.n_locations)\n","      min_d = np.inf\n","      # check if target is reachable\n","      for t_state in self.terminal_states:\n","        dist = self.shortest_path_distances[k][t_state]\n","        min_d = min(min_d, dist)\n","      # min_d = 0  if k is a terminal state\n","      # min_d = -1 if no terminal state is reachable from k\n","      if min_d > 0: break\n","\n","    # reset parameters\n","    self.current_state_id = k\n","    self.s_t = self._tiled_state(self.current_state_id)\n","\n","    self.reward   = 0\n","    self.collided = False\n","    self.terminal = False\n","\n","  def step(self, action):\n","    assert not self.terminal, 'step() called in terminal state'\n","    k = self.current_state_id\n","    if self.transition_graph[k][action] != -1:\n","      self.current_state_id = self.transition_graph[k][action]\n","      if self.terminals[self.current_state_id]:\n","        self.terminal = True\n","        self.collided = False\n","      else:\n","        self.terminal = False\n","        self.collided = False\n","    else:\n","      self.terminal = False\n","      self.collided = True\n","\n","    self.reward = self._reward(self.terminal, self.collided)\n","    self.s_t1 = np.append(self.s_t[:,1:], self.state, axis=1)\n","\n","  def update(self):\n","    self.s_t = self.s_t1\n","\n","  # private methods\n","\n","  def _tiled_state(self, state_id):\n","    k = random.randrange(self.n_feat_per_locaiton)\n","    f = self.h5_file['resnet_feature'][state_id][k][:,np.newaxis]\n","    return np.tile(f, (1, self.history_length))\n","\n","  def _reward(self, terminal, collided):\n","    # positive reward upon task completion\n","    if terminal: return 10.0\n","    # time penalty or collision penalty\n","    return -0.1 if collided else -0.01\n","\n","  # properties\n","\n","  @property\n","  def action_size(self):\n","    # move forward/backward, turn left/right for navigation\n","    return ACTION_SIZE \n","\n","  @property\n","  def action_definitions(self):\n","    action_vocab = [\"MoveForward\", \"RotateRight\", \"RotateLeft\", \"MoveBackward\"]\n","    return action_vocab[:ACTION_SIZE]\n","\n","  @property\n","  def observation(self):\n","    return self.h5_file['observation'][self.current_state_id]\n","\n","  @property\n","  def state(self):\n","    # read from hdf5 cache\n","    k = random.randrange(self.n_feat_per_locaiton)\n","    return self.h5_file['resnet_feature'][self.current_state_id][k][:,np.newaxis]\n","\n","  @property\n","  def target(self):\n","    return self.s_target\n","\n","  @property\n","  def x(self):\n","    return self.locations[self.current_state_id][0]\n","\n","  @property\n","  def z(self):\n","    return self.locations[self.current_state_id][1]\n","\n","  @property\n","  def r(self):\n","    return self.rotations[self.current_state_id]\n","\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1651725468792,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"},"user_tz":240},"id":"6yAh2zBytgFl","outputId":"7ddc70cd-cc19-436d-e9df-a8e38f44c453"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 56.5 ms (started: 2022-05-05 04:37:47 +00:00)\n"]}],"source":["class MyActorCritic(nn.Module):\n","\n","    def __init__(self):\n","        super(MyActorCritic, self).__init__()\n","\n","        # fully connected layer 1\n","\n","        # generic siamese layer: <start>\n","        self.fc1 = nn.Linear(in_features=8192, out_features=512)\n","        self.fc2 = nn.Linear(in_features=1024, out_features=512)\n","        # generic siamese layer: <end>\n","\n","        # Scene specific layer: <start> \n","        # for now we have only one scene speicific layer and only one thread i.e only one target in one scene\n","        self.fc3 = nn.Linear(in_features=512, out_features=512)\n","\n","        self.actions_fc = nn.Linear(in_features=512, out_features=4)\n","        self.value_fc  = nn.Linear(in_features=512, out_features=1)\n","        # Scene specific layer: <end>\n","\n","\n","    def forward(self, input_image_embedding, target_image_embedding):\n","        \n","        # generic siamese latyer\n","        x1 = F.relu(self.fc1(input_image_embedding))\n","        x2 = F.relu(self.fc1(target_image_embedding))\n","\n","        global c_nan\n","        global t_img_embedding\n","\n","        x_combined = torch.cat((x1, x2)) # combine input image emb and target image emb, may need to add axis for concat later\n","\n","        x = F.relu(self.fc2(x_combined))\n","\n","        \n","\n","        # scene specific layer\n","\n","        x = F.relu(self.fc3(x))\n","        x1 = self.actions_fc(x)\n","        # print(f' actions values before softmax:\\n{x1}\\n')\n","        actions_prob = F.softmax(x1, dim=0)\n","        value = self.value_fc(x)\n","\n","        return actions_prob, value\n","\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1651725468793,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"},"user_tz":240},"id":"0CGlF1b0D7is","outputId":"8b3df6ae-eb2a-4235-ff35-d231c9e81f99"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 2.16 ms (started: 2022-05-05 04:37:47 +00:00)\n"]}],"source":["zip_file_name_open = 'h_eval_working.zip'"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1651725468794,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"},"user_tz":240},"id":"P-xJMTNUG29W","outputId":"2c68169c-c970-44fe-cc20-ebe01c796873"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 881 µs (started: 2022-05-05 04:37:47 +00:00)\n"]}],"source":["scene_filename = 'bedroom_04.h5'"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3332,"status":"ok","timestamp":1651725472118,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"},"user_tz":240},"id":"TseSID0M3SMY","outputId":"521fcbe7-2690-418e-d4e9-e40e44435054"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 3.06 s (started: 2022-05-05 04:37:47 +00:00)\n"]}],"source":["!cp -r '/content/gdrive/MyDrive/ESE_650_Project/{zip_file_name_open}' '/content/' "]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3226,"status":"ok","timestamp":1651725475341,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"},"user_tz":240},"id":"1ohUsog5G0xy","outputId":"9f3e6951-8177-406e-fbde-47afd1db844d"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 3.17 s (started: 2022-05-05 04:37:50 +00:00)\n"]}],"source":["!cp -r '/content/gdrive/MyDrive/ESE_650_Project/{scene_filename}' '/content/' "]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3804,"status":"ok","timestamp":1651725479135,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"},"user_tz":240},"id":"tSgf54pFFVxF","outputId":"3c37e429-6110-4a95-f21d-70a87332ea32"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 3.79 s (started: 2022-05-05 04:37:53 +00:00)\n"]}],"source":["%%capture\n","!unzip {zip_file_name_open}"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1362,"status":"ok","timestamp":1651725480486,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"},"user_tz":240},"id":"tynWK4jhzA3L","outputId":"547b21cf-aea3-44f7-8fb6-d1f1b1bb06fb"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n"]},{"output_type":"stream","name":"stdout","text":["time: 1.36 s (started: 2022-05-05 04:37:57 +00:00)\n"]}],"source":["model_load = MyActorCritic()\n","model_load = torch.load('/content/gdrive/MyDrive/ESE_650_Project/models/bedroom_04_myA2C_model_wts_epoch_150000.pth')"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1651725480489,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"},"user_tz":240},"id":"18ySNjFJ0inq","outputId":"9949ba94-f819-43c9-f44e-3755ccebd540"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 859 µs (started: 2022-05-05 04:37:58 +00:00)\n"]}],"source":["device_name = 'cuda:0'\n","# device_name = 'cpu'"]},{"cell_type":"markdown","metadata":{"id":"EWXnJW9j182X"},"source":["# Evaluation of Generalization performance"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2186,"status":"ok","timestamp":1651725482659,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"},"user_tz":240},"id":"nopCw5hhFIed","outputId":"9ca53853-0ab4-47f5-f8f9-8fa8ba80fdc3"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.73 s (started: 2022-05-05 04:37:58 +00:00)\n"]}],"source":["!cp -r '/content/gdrive/MyDrive/ESE_650_Project/bathroom_02.h5' '/content/' "]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2702,"status":"ok","timestamp":1651725485357,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"},"user_tz":240},"id":"14-IF9XBzGnr","outputId":"b27b8e74-916e-4e85-da54-8efe17bc6456"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 3 s (started: 2022-05-05 04:38:00 +00:00)\n"]}],"source":["!cp -r '/content/gdrive/MyDrive/ESE_650_Project/living_room_08.h5' '/content/' "]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4340,"status":"ok","timestamp":1651725489692,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"},"user_tz":240},"id":"kcTQXtFz8q4f","outputId":"1fd29819-29cf-4a6a-ec8e-6c78be297667"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 4.34 s (started: 2022-05-05 04:38:03 +00:00)\n"]}],"source":["!cp -r '/content/gdrive/MyDrive/ESE_650_Project/kitchen_02.h5' '/content/' \n"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1651725489694,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"},"user_tz":240},"id":"Mmne6NQjy7e7","outputId":"8f65e33f-9bc6-423f-e642-dd027f0938d9"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.01 ms (started: 2022-05-05 04:38:07 +00:00)\n"]}],"source":["trained_scene = 'bedroom_04.h5'"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1651725489696,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"},"user_tz":240},"id":"DJwyeu8t8q4f","outputId":"487c4fdf-5012-4dd6-eefa-100d4c372b55"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.1 ms (started: 2022-05-05 04:38:08 +00:00)\n"]}],"source":["other_scenes = ['kitchen_02', 'bathroom_02', 'living_room_08']"]},{"cell_type":"markdown","metadata":{"id":"gJja3Lu0eitQ"},"source":["## Evaluation on trained targets in the scene"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":350939,"status":"ok","timestamp":1651725840626,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"},"user_tz":240},"id":"fHiNw18Bz9-4","outputId":"b30374e4-0f04-4a93-c101-78f3fa0c2693"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","bedroom_04.h5\n","terminal state id: 134\n","\n","\t\tnum_iter : 0\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 9\n","\t\tfinal state to target state shortest dist: 9\n","\n","\t\tnum_iter : 1\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 9\n","\t\tfinal state to target state shortest dist: 9\n","\n","\t\tnum_iter : 2\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 5\n","\t\tfinal state to target state shortest dist: 5\n","\n","\t\tnum_iter : 3\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 13\n","\t\tfinal state to target state shortest dist: 13\n","\n","\t\tnum_iter : 4\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 17\n","\t\tfinal state to target state shortest dist: 17\n","\n","\taverage episode length = 10002.0\n","\taverage episode reward = -100.02000000001426\n","\n","bedroom_04.h5\n","terminal state id: 264\n","\n","\t\tnum_iter : 0\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 9\n","\t\tfinal state to target state shortest dist: 9\n","\n","\t\tnum_iter : 1\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 6\n","\t\tfinal state to target state shortest dist: 6\n","\n","\t\tnum_iter : 2\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 10\n","\t\tfinal state to target state shortest dist: 10\n","\n","\t\tnum_iter : 3\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 5\n","\t\tfinal state to target state shortest dist: 5\n","\n","\t\tnum_iter : 4\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 7\n","\t\tfinal state to target state shortest dist: 7\n","\n","\taverage episode length = 10002.0\n","\taverage episode reward = -100.02000000001426\n","\n","bedroom_04.h5\n","terminal state id: 320\n","\n","\t\tnum_iter : 0\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 5\n","\t\tfinal state to target state shortest dist: 5\n","\n","\t\tnum_iter : 1\n","\t\tepisode_length : 1\n","\t\tepisode_reward : 10.0\n","\t\tinital state to target state shortest dist: 1\n","\t\tfinal state to target state shortest dist: 0\n","\n","\t\tnum_iter : 2\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 6\n","\t\tfinal state to target state shortest dist: 6\n","\n","\t\tnum_iter : 3\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 12\n","\t\tfinal state to target state shortest dist: 12\n","\n","\t\tnum_iter : 4\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 24\n","\t\tfinal state to target state shortest dist: 24\n","\n","\taverage episode length = 8001.8\n","\taverage episode reward = -78.01600000001142\n","\n","bedroom_04.h5\n","terminal state id: 384\n","\n","\t\tnum_iter : 0\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 18\n","\t\tfinal state to target state shortest dist: 18\n","\n","\t\tnum_iter : 1\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 9\n","\t\tfinal state to target state shortest dist: 9\n","\n","\t\tnum_iter : 2\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 8\n","\t\tfinal state to target state shortest dist: 6\n","\n","\t\tnum_iter : 3\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 23\n","\t\tfinal state to target state shortest dist: 23\n","\n","\t\tnum_iter : 4\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 19\n","\t\tfinal state to target state shortest dist: 19\n","\n","\taverage episode length = 10002.0\n","\taverage episode reward = -100.02000000001426\n","\n","bedroom_04.h5\n","terminal state id: 387\n","\n","\t\tnum_iter : 0\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 18\n","\t\tfinal state to target state shortest dist: 18\n","\n","\t\tnum_iter : 1\n","\t\tepisode_length : 3\n","\t\tepisode_reward : 9.98\n","\t\tinital state to target state shortest dist: 1\n","\t\tfinal state to target state shortest dist: 0\n","\n","\t\tnum_iter : 2\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 11\n","\t\tfinal state to target state shortest dist: 11\n","\n","\t\tnum_iter : 3\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 25\n","\t\tfinal state to target state shortest dist: 25\n","\n","\t\tnum_iter : 4\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 26\n","\t\tfinal state to target state shortest dist: 26\n","\n","\taverage episode length = 8002.2\n","\taverage episode reward = -78.02000000001141\n","\n","average scene length = 9202.0\n","average scene reward = -91.21920000001312\n","time: 5min 50s (started: 2022-05-05 04:38:08 +00:00)\n"]}],"source":["# for trained scenes in task list\n","\n","model_load.to(device_name)\n","\n","\n","target_list = TASK_LIST['bedroom_04']\n","\n","total_episode_len_scene_tr = []\n","total_episode_reward_scene_tr = []\n","\n","\n","for target_s in target_list:\n","\n","    target_s_int = int(target_s)\n","\n","    \n","\n","    eval_env = THORDiscreteEnvironment({\n","    'random_start': True,\n","    'scene_name': trained_scene,\n","    'h5_file_path': '%s'%trained_scene,\n","    'terminal_state_id' : target_s_int\n","    })\n","\n","    print()\n","    print('%s'%trained_scene)\n","    print(f'terminal state id: {eval_env.terminal_state_id}')\n","\n","    total_episode_len_eps_tr = np.zeros(NUM_EVAL_EPISODES)\n","    total_episode_reward_eps_tr = np.zeros(NUM_EVAL_EPISODES)\n","\n","    for episode in range(NUM_EVAL_EPISODES):\n","\n","\n","        model_load.eval()\n","        eval_env.reset()\n","\n","        actions = []\n","\n","        final_state = None\n","        inital_state_id = eval_env.current_state_id\n","\n","        terminal_reached = False\n","\n","        episode_length = 0\n","        episode_reward = 0\n","        local_t = 0\n","\n","        terminal = False\n","        inital_state_to_target_dist = eval_env.shortest_path_distances[inital_state_id][eval_env.terminal_state_id]\n","        \n","\n","        while not terminal:\n","\n","            torch_s_t = torch.from_numpy(eval_env.s_t).float().flatten()  # state embedding in torch convertinng from 2048 x 4, to 8192 x 1\n","            torch_target = torch.from_numpy(eval_env.target).float().flatten()  # target embedding in torch\n","\n","            torch_s_t = torch_s_t.to(device_name)\n","            torch_target = torch_target.to(device_name)\n","\n","            actions_prob, value = model_load(torch_s_t, torch_target)\n","\n","            action = torch.argmax(actions_prob)  # choose best action\n","\n","            # store required items\n","            actions.append(action)\n","\n","            # process game\n","            eval_env.step(action)\n","\n","            # receive game result\n","            reward = eval_env.reward\n","            terminal = eval_env.terminal\n","\n","            # ad-hoc reward for navigation\n","            reward = 10.0 if terminal else -0.01\n","            if episode_length > 1e4: terminal = True\n","\n","            episode_reward += reward\n","            episode_length += 1\n","            # episode_max_q = max(episode_max_q, np.max(value_)) !!!! DO Q VALUE CLIPPING LATER !!!!!!!\n","\n","            local_t += 1\n","\n","            # s_t1 -> s_t\n","            eval_env.update()\n","\n","            if terminal:\n","                final_state = eval_env.s_t[0]\n","                terminal_end = True\n","                break\n","\n","        \n","        final_state_to_target_dist = eval_env.shortest_path_distances[eval_env.current_state_id][eval_env.terminal_state_id]\n","        print(f'\\n\\t\\tnum_iter : {episode}\\n\\t\\tepisode_length : {episode_length}')\n","        print(f'\\t\\tepisode_reward : {episode_reward}')\n","        print(f'\\t\\tinital state to target state shortest dist: {inital_state_to_target_dist}')\n","        # print(f'shortest dist from episode termination state to goal state: {dist}')\n","        print(f'\\t\\tfinal state to target state shortest dist: {final_state_to_target_dist}')\n","        \n","        total_episode_len_eps_tr[episode] = episode_length\n","        total_episode_reward_eps_tr[episode] = episode_reward\n","\n","        \n","    print()\n","    print(f'\\taverage episode length = {total_episode_len_eps_tr.mean()}')\n","    print(f'\\taverage episode reward = {total_episode_reward_eps_tr.mean()}')\n","    total_episode_len_scene_tr.append(total_episode_len_eps_tr.mean().item())\n","    total_episode_reward_scene_tr.append(total_episode_reward_eps_tr.mean().item())\n","\n","print()\n","print(f'average scene length = {np.array(total_episode_len_scene_tr).mean()}')\n","print(f'average scene reward = {np.array(total_episode_reward_scene_tr).mean()}')\n","\n","\n","    "]},{"cell_type":"markdown","source":["####Observe that the agent keeps rotating about in the initialized position without any forward or backward movement\n","\n","Action space\n","\n","0 : Move forward\n","\n","1 : Rotate right\n","\n","2 : Rotate left\n","\n","3 : Move backward"],"metadata":{"id":"2FDk1usDdPOY"}},{"cell_type":"code","source":["# for trained scene in task list observe that the agent rotates about a fixed position\n","\n","model_load.to(device_name)\n","\n","\n","target_list = TASK_LIST['bedroom_04']\n","\n","total_episode_len_scene_tr = []\n","total_episode_reward_scene_tr = []\n","\n","\n","for target_s in target_list:\n","\n","    target_s_int = int(target_s)\n","\n","    \n","\n","    eval_env = THORDiscreteEnvironment({\n","    'random_start': True,\n","    'scene_name': trained_scene,\n","    'h5_file_path': '%s'%trained_scene,\n","    'terminal_state_id' : target_s_int\n","    })\n","\n","    print()\n","    print('%s'%trained_scene)\n","    print(f'terminal state id: {eval_env.terminal_state_id}')\n","\n","    total_episode_len_eps_tr = np.zeros(NUM_EVAL_EPISODES)\n","    total_episode_reward_eps_tr = np.zeros(NUM_EVAL_EPISODES)\n","\n","    for episode in range(1):\n","\n","\n","        model_load.eval()\n","        eval_env.reset()\n","\n","        actions = []\n","\n","        final_state = None\n","        inital_state_id = eval_env.current_state_id\n","\n","        terminal_reached = False\n","\n","        episode_length = 0\n","        episode_reward = 0\n","        local_t = 0\n","\n","        terminal = False\n","        inital_state_to_target_dist = eval_env.shortest_path_distances[inital_state_id][eval_env.terminal_state_id]\n","        \n","\n","        while not terminal:\n","\n","            print()\n","            print(f'current state before action: {eval_env.current_state_id}')\n","\n","            torch_s_t = torch.from_numpy(eval_env.s_t).float().flatten()  # state embedding in torch convertinng from 2048 x 4, to 8192 x 1\n","            torch_target = torch.from_numpy(eval_env.target).float().flatten()  # target embedding in torch\n","\n","            torch_s_t = torch_s_t.to(device_name)\n","            torch_target = torch_target.to(device_name)\n","\n","            actions_prob, value = model_load(torch_s_t, torch_target)\n","\n","            action = torch.argmax(actions_prob)  # choose best action\n","\n","            # store required items\n","            actions.append(action.detach().cpu().item())\n","\n","            # process game\n","            eval_env.step(action)\n","\n","            # receive game result\n","            reward = eval_env.reward\n","            terminal = eval_env.terminal\n","\n","            # ad-hoc reward for navigation\n","            reward = 10.0 if terminal else -0.01\n","            if episode_length > 1e2: terminal = True\n","\n","            episode_reward += reward\n","            episode_length += 1\n","            # episode_max_q = max(episode_max_q, np.max(value_)) !!!! DO Q VALUE CLIPPING LATER !!!!!!!\n","\n","            local_t += 1\n","\n","            # s_t1 -> s_t\n","            eval_env.update()\n","\n","            print(f'current action: {action}')\n","            print(f'agent current state id after action:{eval_env.current_state_id}')\n","            print(f'agent current position x {eval_env.x}')\n","            print(f'agent current position z {eval_env.z}')\n","            print(f'agent current orientation {eval_env.r}')\n","\n","            if terminal:\n","                final_state = eval_env.s_t[0]\n","                terminal_end = True\n","                break\n","\n","            \n","\n","\n","        \n","        final_state_to_target_dist = eval_env.shortest_path_distances[eval_env.current_state_id][eval_env.terminal_state_id]\n","        print(f'\\nnum_iter : {episode}\\nepisode_length : {episode_length}')\n","        print(f'episode_reward : {episode_reward}')\n","        print(f'inital state to target state shortest dist: {inital_state_to_target_dist}')\n","        # print(f'shortest dist from episode termination state to goal state: {dist}')\n","        print(f'final state to target state shortest dist: {final_state_to_target_dist}')\n","        \n","        total_episode_len_eps_tr[episode] = episode_length\n","        total_episode_reward_eps_tr[episode] = episode_reward\n","\n","    from pprint import pprint\n","    print()\n","    print('first 10 actions:')\n","    pprint(actions[:10])\n","    print()\n","    print('last 10 actions:')\n","\n","    pprint(actions[-10:-1])\n","\n","    break\n","    "],"metadata":{"id":"N4jl4l_vdFB4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651726394417,"user_tz":240,"elapsed":954,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}},"outputId":"a875c74d-3c40-49e2-9589-815e7d463388"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","bedroom_04.h5\n","terminal state id: 134\n","\n","current state before action: 348\n","current action: 1\n","agent current state id after action:349\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 90\n","\n","current state before action: 349\n","current action: 1\n","agent current state id after action:350\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 180\n","\n","current state before action: 350\n","current action: 1\n","agent current state id after action:351\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 270\n","\n","current state before action: 351\n","current action: 1\n","agent current state id after action:348\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 0\n","\n","current state before action: 348\n","current action: 1\n","agent current state id after action:349\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 90\n","\n","current state before action: 349\n","current action: 1\n","agent current state id after action:350\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 180\n","\n","current state before action: 350\n","current action: 1\n","agent current state id after action:351\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 270\n","\n","current state before action: 351\n","current action: 1\n","agent current state id after action:348\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 0\n","\n","current state before action: 348\n","current action: 1\n","agent current state id after action:349\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 90\n","\n","current state before action: 349\n","current action: 1\n","agent current state id after action:350\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 180\n","\n","current state before action: 350\n","current action: 1\n","agent current state id after action:351\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 270\n","\n","current state before action: 351\n","current action: 1\n","agent current state id after action:348\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 0\n","\n","current state before action: 348\n","current action: 1\n","agent current state id after action:349\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 90\n","\n","current state before action: 349\n","current action: 1\n","agent current state id after action:350\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 180\n","\n","current state before action: 350\n","current action: 1\n","agent current state id after action:351\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 270\n","\n","current state before action: 351\n","current action: 1\n","agent current state id after action:348\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 0\n","\n","current state before action: 348\n","current action: 1\n","agent current state id after action:349\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 90\n","\n","current state before action: 349\n","current action: 1\n","agent current state id after action:350\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 180\n","\n","current state before action: 350\n","current action: 1\n","agent current state id after action:351\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 270\n","\n","current state before action: 351\n","current action: 1\n","agent current state id after action:348\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 0\n","\n","current state before action: 348\n","current action: 1\n","agent current state id after action:349\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 90\n","\n","current state before action: 349\n","current action: 1\n","agent current state id after action:350\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 180\n","\n","current state before action: 350\n","current action: 1\n","agent current state id after action:351\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 270\n","\n","current state before action: 351\n","current action: 1\n","agent current state id after action:348\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 0\n","\n","current state before action: 348\n","current action: 1\n","agent current state id after action:349\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 90\n","\n","current state before action: 349\n","current action: 1\n","agent current state id after action:350\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 180\n","\n","current state before action: 350\n","current action: 1\n","agent current state id after action:351\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 270\n","\n","current state before action: 351\n","current action: 1\n","agent current state id after action:348\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 0\n","\n","current state before action: 348\n","current action: 1\n","agent current state id after action:349\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 90\n","\n","current state before action: 349\n","current action: 1\n","agent current state id after action:350\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 180\n","\n","current state before action: 350\n","current action: 1\n","agent current state id after action:351\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 270\n","\n","current state before action: 351\n","current action: 1\n","agent current state id after action:348\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 0\n","\n","current state before action: 348\n","current action: 1\n","agent current state id after action:349\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 90\n","\n","current state before action: 349\n","current action: 1\n","agent current state id after action:350\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 180\n","\n","current state before action: 350\n","current action: 1\n","agent current state id after action:351\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 270\n","\n","current state before action: 351\n","current action: 1\n","agent current state id after action:348\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 0\n","\n","current state before action: 348\n","current action: 1\n","agent current state id after action:349\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 90\n","\n","current state before action: 349\n","current action: 1\n","agent current state id after action:350\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 180\n","\n","current state before action: 350\n","current action: 1\n","agent current state id after action:351\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 270\n","\n","current state before action: 351\n","current action: 1\n","agent current state id after action:348\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 0\n","\n","current state before action: 348\n","current action: 1\n","agent current state id after action:349\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 90\n","\n","current state before action: 349\n","current action: 1\n","agent current state id after action:350\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 180\n","\n","current state before action: 350\n","current action: 1\n","agent current state id after action:351\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 270\n","\n","current state before action: 351\n","current action: 1\n","agent current state id after action:348\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 0\n","\n","current state before action: 348\n","current action: 1\n","agent current state id after action:349\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 90\n","\n","current state before action: 349\n","current action: 1\n","agent current state id after action:350\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 180\n","\n","current state before action: 350\n","current action: 1\n","agent current state id after action:351\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 270\n","\n","current state before action: 351\n","current action: 1\n","agent current state id after action:348\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 0\n","\n","current state before action: 348\n","current action: 1\n","agent current state id after action:349\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 90\n","\n","current state before action: 349\n","current action: 1\n","agent current state id after action:350\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 180\n","\n","current state before action: 350\n","current action: 1\n","agent current state id after action:351\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 270\n","\n","current state before action: 351\n","current action: 1\n","agent current state id after action:348\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 0\n","\n","current state before action: 348\n","current action: 1\n","agent current state id after action:349\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 90\n","\n","current state before action: 349\n","current action: 1\n","agent current state id after action:350\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 180\n","\n","current state before action: 350\n","current action: 1\n","agent current state id after action:351\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 270\n","\n","current state before action: 351\n","current action: 1\n","agent current state id after action:348\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 0\n","\n","current state before action: 348\n","current action: 1\n","agent current state id after action:349\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 90\n","\n","current state before action: 349\n","current action: 1\n","agent current state id after action:350\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 180\n","\n","current state before action: 350\n","current action: 1\n","agent current state id after action:351\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 270\n","\n","current state before action: 351\n","current action: 1\n","agent current state id after action:348\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 0\n","\n","current state before action: 348\n","current action: 1\n","agent current state id after action:349\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 90\n","\n","current state before action: 349\n","current action: 1\n","agent current state id after action:350\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 180\n","\n","current state before action: 350\n","current action: 1\n","agent current state id after action:351\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 270\n","\n","current state before action: 351\n","current action: 1\n","agent current state id after action:348\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 0\n","\n","current state before action: 348\n","current action: 1\n","agent current state id after action:349\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 90\n","\n","current state before action: 349\n","current action: 1\n","agent current state id after action:350\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 180\n","\n","current state before action: 350\n","current action: 1\n","agent current state id after action:351\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 270\n","\n","current state before action: 351\n","current action: 1\n","agent current state id after action:348\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 0\n","\n","current state before action: 348\n","current action: 1\n","agent current state id after action:349\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 90\n","\n","current state before action: 349\n","current action: 1\n","agent current state id after action:350\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 180\n","\n","current state before action: 350\n","current action: 1\n","agent current state id after action:351\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 270\n","\n","current state before action: 351\n","current action: 1\n","agent current state id after action:348\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 0\n","\n","current state before action: 348\n","current action: 1\n","agent current state id after action:349\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 90\n","\n","current state before action: 349\n","current action: 1\n","agent current state id after action:350\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 180\n","\n","current state before action: 350\n","current action: 1\n","agent current state id after action:351\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 270\n","\n","current state before action: 351\n","current action: 1\n","agent current state id after action:348\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 0\n","\n","current state before action: 348\n","current action: 1\n","agent current state id after action:349\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 90\n","\n","current state before action: 349\n","current action: 1\n","agent current state id after action:350\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 180\n","\n","current state before action: 350\n","current action: 1\n","agent current state id after action:351\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 270\n","\n","current state before action: 351\n","current action: 1\n","agent current state id after action:348\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 0\n","\n","current state before action: 348\n","current action: 1\n","agent current state id after action:349\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 90\n","\n","current state before action: 349\n","current action: 1\n","agent current state id after action:350\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 180\n","\n","current state before action: 350\n","current action: 1\n","agent current state id after action:351\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 270\n","\n","current state before action: 351\n","current action: 1\n","agent current state id after action:348\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 0\n","\n","current state before action: 348\n","current action: 1\n","agent current state id after action:349\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 90\n","\n","current state before action: 349\n","current action: 1\n","agent current state id after action:350\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 180\n","\n","current state before action: 350\n","current action: 1\n","agent current state id after action:351\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 270\n","\n","current state before action: 351\n","current action: 1\n","agent current state id after action:348\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 0\n","\n","current state before action: 348\n","current action: 1\n","agent current state id after action:349\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 90\n","\n","current state before action: 349\n","current action: 1\n","agent current state id after action:350\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 180\n","\n","current state before action: 350\n","current action: 1\n","agent current state id after action:351\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 270\n","\n","current state before action: 351\n","current action: 1\n","agent current state id after action:348\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 0\n","\n","current state before action: 348\n","current action: 1\n","agent current state id after action:349\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 90\n","\n","current state before action: 349\n","current action: 1\n","agent current state id after action:350\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 180\n","\n","current state before action: 350\n","current action: 1\n","agent current state id after action:351\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 270\n","\n","current state before action: 351\n","current action: 1\n","agent current state id after action:348\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 0\n","\n","current state before action: 348\n","current action: 1\n","agent current state id after action:349\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 90\n","\n","current state before action: 349\n","current action: 1\n","agent current state id after action:350\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 180\n","\n","current state before action: 350\n","current action: 1\n","agent current state id after action:351\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 270\n","\n","current state before action: 351\n","current action: 1\n","agent current state id after action:348\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 0\n","\n","current state before action: 348\n","current action: 1\n","agent current state id after action:349\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 90\n","\n","current state before action: 349\n","current action: 1\n","agent current state id after action:350\n","agent current position x 8.0\n","agent current position z 6.0\n","agent current orientation 180\n","\n","num_iter : 0\n","episode_length : 102\n","episode_reward : -1.0200000000000007\n","inital state to target state shortest dist: 8\n","final state to target state shortest dist: 8\n","\n","first 10 actions:\n","[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","\n","last 10 actions:\n","[1, 1, 1, 1, 1, 1, 1, 1, 1]\n","time: 895 ms (started: 2022-05-05 04:53:12 +00:00)\n"]}]},{"cell_type":"markdown","metadata":{"id":"ujaxeWTBe18m"},"source":["## Evaluation on targets different from trained targets in trained scene"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1651726406265,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"},"user_tz":240},"id":"JSDEpWCb7K2c","outputId":"e3790b07-c5d3-41e7-a8e2-85e60a65ed8b"},"outputs":[{"output_type":"stream","name":"stdout","text":["bedroom_04.h5\n","bedroom_04\n","265\n","time: 12.1 ms (started: 2022-05-05 04:53:24 +00:00)\n"]}],"source":["## load scene into environment\n","scene_name = 'bedroom_04'\n","print('%s.h5'%scene_name)\n","\n","eval_env = THORDiscreteEnvironment({\n","'random_start': True,\n","'scene_name': scene_name,\n","'h5_file_path': '%s.h5'%scene_name,\n","'terminal_state_id' : 265\n","})\n","\n","print(eval_env.scene_name)\n","print(eval_env.terminal_state_id)"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Z3ETrs8zPQL","outputId":"63010685-fa29-4c73-d681-1c4869439a63","executionInfo":{"status":"ok","timestamp":1651726850738,"user_tz":240,"elapsed":443248,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["bedroom_04\n","265\n","\n","bedroom_04.h5\n","terminal state id: 133\n","\n","\t\tnum_iter : 0\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 8\n","\t\tfinal state to target state shortest dist: 8\n","\n","\t\tnum_iter : 1\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 7\n","\t\tfinal state to target state shortest dist: 7\n","\n","\t\tnum_iter : 2\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 3\n","\t\tfinal state to target state shortest dist: 3\n","\n","\t\tnum_iter : 3\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 5\n","\t\tfinal state to target state shortest dist: 5\n","\n","\t\tnum_iter : 4\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 2\n","\t\tfinal state to target state shortest dist: 4\n","\n","\taverage episode length = 10002.0\n","\taverage episode reward = -100.02000000001426\n","\n","bedroom_04.h5\n","terminal state id: 129\n","\n","\t\tnum_iter : 0\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 10\n","\t\tfinal state to target state shortest dist: 10\n","\n","\t\tnum_iter : 1\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 13\n","\t\tfinal state to target state shortest dist: 13\n","\n","\t\tnum_iter : 2\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 10\n","\t\tfinal state to target state shortest dist: 10\n","\n","\t\tnum_iter : 3\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 9\n","\t\tfinal state to target state shortest dist: 9\n","\n","\t\tnum_iter : 4\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 13\n","\t\tfinal state to target state shortest dist: 13\n","\n","\taverage episode length = 10002.0\n","\taverage episode reward = -100.02000000001426\n","\n","bedroom_04.h5\n","terminal state id: 196\n","\n","\t\tnum_iter : 0\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 9\n","\t\tfinal state to target state shortest dist: 9\n","\n","\t\tnum_iter : 1\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 16\n","\t\tfinal state to target state shortest dist: 16\n","\n","\t\tnum_iter : 2\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 10\n","\t\tfinal state to target state shortest dist: 10\n","\n","\t\tnum_iter : 3\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 14\n","\t\tfinal state to target state shortest dist: 14\n","\n","\t\tnum_iter : 4\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 2\n","\t\tfinal state to target state shortest dist: 2\n","\n","\taverage episode length = 10002.0\n","\taverage episode reward = -100.02000000001426\n","\n","bedroom_04.h5\n","terminal state id: 197\n","\n","\t\tnum_iter : 0\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 7\n","\t\tfinal state to target state shortest dist: 7\n","\n","\t\tnum_iter : 1\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 5\n","\t\tfinal state to target state shortest dist: 5\n","\n","\t\tnum_iter : 2\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 10\n","\t\tfinal state to target state shortest dist: 10\n","\n","\t\tnum_iter : 3\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 8\n","\t\tfinal state to target state shortest dist: 8\n","\n","\t\tnum_iter : 4\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 8\n","\t\tfinal state to target state shortest dist: 8\n","\n","\taverage episode length = 10002.0\n","\taverage episode reward = -100.02000000001426\n","\n","bedroom_04.h5\n","terminal state id: 288\n","\n","\t\tnum_iter : 0\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 8\n","\t\tfinal state to target state shortest dist: 8\n","\n","\t\tnum_iter : 1\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 11\n","\t\tfinal state to target state shortest dist: 11\n","\n","\t\tnum_iter : 2\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 18\n","\t\tfinal state to target state shortest dist: 18\n","\n","\t\tnum_iter : 3\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 9\n","\t\tfinal state to target state shortest dist: 9\n","\n","\t\tnum_iter : 4\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 16\n","\t\tfinal state to target state shortest dist: 16\n","\n","\taverage episode length = 10002.0\n","\taverage episode reward = -100.02000000001426\n","\n","bedroom_04.h5\n","terminal state id: 220\n","\n","\t\tnum_iter : 0\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 8\n","\t\tfinal state to target state shortest dist: 8\n","\n","\t\tnum_iter : 1\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 3\n","\t\tfinal state to target state shortest dist: 3\n","\n","\t\tnum_iter : 2\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 11\n","\t\tfinal state to target state shortest dist: 11\n","\n","\t\tnum_iter : 3\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 7\n","\t\tfinal state to target state shortest dist: 7\n","\n","\t\tnum_iter : 4\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 3\n","\t\tfinal state to target state shortest dist: 3\n","\n","\taverage episode length = 10002.0\n","\taverage episode reward = -100.02000000001426\n","\n","dist from target : 1\n","average episode length = 10002.0\n","average episode reward = -100.02000000001426\n","dist from target : 2\n","average episode length = 10002.0\n","average episode reward = -100.02000000001426\n","time: 7min 23s (started: 2022-05-05 04:53:25 +00:00)\n"]}],"source":["# on other targets in trained scene\n","\n","\n","print(eval_env.scene_name)\n","print(eval_env.terminal_state_id)\n","\n","\n","model_load.to(device_name)\n","\n","\n","target_list = TASK_LIST['bedroom_04']\n","\n","nearest_targets_dict = {}\n","\n","# dist 1 and 2\n","\n","target_list = target_list[:3] # check for 3 targets\n","nearest_dist = [1, 2]\n","\n","\n","for target in target_list:\n","\n","    nearest_targets_dict[target] = []\n","\n","    target = int(target)\n","\n","    for n_dist in nearest_dist:\n","\n","        eval_env = THORDiscreteEnvironment({\n","        'random_start': True,\n","        'scene_name': trained_scene,\n","        'h5_file_path': '%s'%trained_scene,\n","        'terminal_state_id' : target\n","        })\n","\n","        for i in range(eval_env.n_locations):\n","\n","            if eval_env.shortest_path_distances[target][i] == n_dist:\n","                nearest_targets_dict[str(target)].append(i)\n","                break\n","\n","# print(nearest_targets_dict)\n","# print(eval_env.shortest_path_distances[264][196])\n","\n","\n","avg_len_dict = {1:[], 2:[]}\n","avg_reward_dict = {1:[], 2:[]}\n","\n","for t in nearest_targets_dict.keys():\n","\n","    target_list =  nearest_targets_dict[t]\n","    i=1\n","\n","\n","    for target_s in target_list:\n","\n","        target_s_int = int(target_s)\n","\n","        \n","\n","        eval_env = THORDiscreteEnvironment({\n","        'random_start': True,\n","        'scene_name': trained_scene,\n","        'h5_file_path': '%s'%trained_scene,\n","        'terminal_state_id' : target_s_int\n","        })\n","\n","        print()\n","        print('%s'%trained_scene)\n","        print(f'terminal state id: {eval_env.terminal_state_id}')\n","\n","        total_episode_len_eps_tr = np.zeros(NUM_EVAL_EPISODES)\n","        total_episode_reward_eps_tr = np.zeros(NUM_EVAL_EPISODES)\n","\n","        for episode in range(NUM_EVAL_EPISODES):\n","\n","\n","            model_load.eval()\n","            eval_env.reset()\n","\n","            actions = []\n","\n","            final_state = None\n","            inital_state_id = eval_env.current_state_id\n","\n","            terminal_reached = False\n","\n","            episode_length = 0\n","            episode_reward = 0\n","            local_t = 0\n","\n","            terminal = False\n","            inital_state_to_target_dist = eval_env.shortest_path_distances[inital_state_id][eval_env.terminal_state_id]\n","            \n","\n","            while not terminal:\n","\n","                torch_s_t = torch.from_numpy(eval_env.s_t).float().flatten()  # state embedding in torch convertinng from 2048 x 4, to 8192 x 1\n","                torch_target = torch.from_numpy(eval_env.target).float().flatten()  # target embedding in torch\n","\n","                torch_s_t = torch_s_t.to(device_name)\n","                torch_target = torch_target.to(device_name)\n","\n","                actions_prob, value = model_load(torch_s_t, torch_target)\n","\n","                action = torch.argmax(actions_prob)  # choose best action\n","\n","                # store required items\n","                actions.append(action)\n","\n","                # process game\n","                eval_env.step(action)\n","\n","                # receive game result\n","                reward = eval_env.reward\n","                terminal = eval_env.terminal\n","\n","                # ad-hoc reward for navigation\n","                reward = 10.0 if terminal else -0.01\n","                if episode_length > 1e4: terminal = True\n","\n","                episode_reward += reward\n","                episode_length += 1\n","                # episode_max_q = max(episode_max_q, np.max(value_)) !!!! DO Q VALUE CLIPPING LATER !!!!!!!\n","\n","                local_t += 1\n","\n","                # s_t1 -> s_t\n","                eval_env.update()\n","\n","                if terminal:\n","                    final_state = eval_env.s_t[0]\n","                    terminal_end = True\n","                    break\n","\n","            \n","            final_state_to_target_dist = eval_env.shortest_path_distances[eval_env.current_state_id][eval_env.terminal_state_id]\n","            print(f'\\n\\t\\tnum_iter : {episode}\\n\\t\\tepisode_length : {episode_length}')\n","            print(f'\\t\\tepisode_reward : {episode_reward}')\n","            print(f'\\t\\tinital state to target state shortest dist: {inital_state_to_target_dist}')\n","            # print(f'shortest dist from episode termination state to goal state: {dist}')\n","            print(f'\\t\\tfinal state to target state shortest dist: {final_state_to_target_dist}')\n","            \n","            total_episode_len_eps_tr[episode] = episode_length\n","            total_episode_reward_eps_tr[episode] = episode_reward\n","\n","            \n","        print()\n","        print(f'\\taverage episode length = {total_episode_len_eps_tr.mean()}')\n","        print(f'\\taverage episode reward = {total_episode_reward_eps_tr.mean()}')\n","        total_episode_len_scene_tr.append(total_episode_len_eps_tr.mean().item())\n","        total_episode_reward_scene_tr.append(total_episode_reward_eps_tr.mean().item())\n","\n","        avg_len_dict[i].append(total_episode_len_eps_tr.mean())\n","        avg_reward_dict[i].append(total_episode_reward_eps_tr.mean())\n","    \n","        i += 1\n","\n","\n","print()\n","\n","for k in avg_len_dict.keys():\n","    print(f'dist from target : {k}')\n","    print(f'average episode length = {np.array(avg_len_dict[k]).mean()}')\n","    print(f'average episode reward = {np.array(avg_reward_dict[k]).mean()}')\n"]},{"cell_type":"markdown","metadata":{"id":"t9VuQNMje_mS"},"source":["## Evaluation on targets in different scene"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"RvAuQKATGJiR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651727948616,"user_tz":240,"elapsed":1097925,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}},"outputId":"01b478a1-087c-4466-f253-062658ce04ec"},"outputs":[{"output_type":"stream","name":"stdout","text":["kitchen_02\n","\n","kitchen_02\n","terminal state id: 90\n","\n","\t\tnum_iter : 0\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 18\n","\t\tfinal state to target state shortest dist: 18\n","\n","\t\tnum_iter : 1\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 4\n","\t\tfinal state to target state shortest dist: 4\n","\n","\t\tnum_iter : 2\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 13\n","\t\tfinal state to target state shortest dist: 13\n","\n","\t\tnum_iter : 3\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 17\n","\t\tfinal state to target state shortest dist: 17\n","\n","\t\tnum_iter : 4\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 15\n","\t\tfinal state to target state shortest dist: 15\n","\n","\taverage episode length = 10002.0\n","\taverage episode reward = -100.02000000001426\n","\n","kitchen_02\n","terminal state id: 136\n","\n","\t\tnum_iter : 0\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 29\n","\t\tfinal state to target state shortest dist: 29\n","\n","\t\tnum_iter : 1\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 21\n","\t\tfinal state to target state shortest dist: 21\n","\n","\t\tnum_iter : 2\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 5\n","\t\tfinal state to target state shortest dist: 5\n","\n","\t\tnum_iter : 3\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 21\n","\t\tfinal state to target state shortest dist: 21\n","\n","\t\tnum_iter : 4\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 15\n","\t\tfinal state to target state shortest dist: 15\n","\n","\taverage episode length = 10002.0\n","\taverage episode reward = -100.02000000001426\n","\n","kitchen_02\n","terminal state id: 157\n","\n","\t\tnum_iter : 0\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 37\n","\t\tfinal state to target state shortest dist: 37\n","\n","\t\tnum_iter : 1\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 4\n","\t\tfinal state to target state shortest dist: 4\n","\n","\t\tnum_iter : 2\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 11\n","\t\tfinal state to target state shortest dist: 11\n","\n","\t\tnum_iter : 3\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 23\n","\t\tfinal state to target state shortest dist: 23\n","\n","\t\tnum_iter : 4\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 30\n","\t\tfinal state to target state shortest dist: 30\n","\n","\taverage episode length = 10002.0\n","\taverage episode reward = -100.02000000001426\n","\n","kitchen_02\n","terminal state id: 207\n","\n","\t\tnum_iter : 0\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 22\n","\t\tfinal state to target state shortest dist: 22\n","\n","\t\tnum_iter : 1\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 8\n","\t\tfinal state to target state shortest dist: 8\n","\n","\t\tnum_iter : 2\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 4\n","\t\tfinal state to target state shortest dist: 4\n","\n","\t\tnum_iter : 3\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 24\n","\t\tfinal state to target state shortest dist: 24\n","\n","\t\tnum_iter : 4\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 2\n","\t\tfinal state to target state shortest dist: 2\n","\n","\taverage episode length = 10002.0\n","\taverage episode reward = -100.02000000001426\n","\n","kitchen_02\n","terminal state id: 329\n","\n","\t\tnum_iter : 0\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 12\n","\t\tfinal state to target state shortest dist: 12\n","\n","\t\tnum_iter : 1\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 17\n","\t\tfinal state to target state shortest dist: 17\n","\n","\t\tnum_iter : 2\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 16\n","\t\tfinal state to target state shortest dist: 16\n","\n","\t\tnum_iter : 3\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 17\n","\t\tfinal state to target state shortest dist: 17\n","\n","\t\tnum_iter : 4\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 4\n","\t\tfinal state to target state shortest dist: 4\n","\n","\taverage episode length = 10002.0\n","\taverage episode reward = -100.02000000001426\n","bathroom_02\n","\n","bathroom_02\n","terminal state id: 26\n","\n","\t\tnum_iter : 0\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 11\n","\t\tfinal state to target state shortest dist: 11\n","\n","\t\tnum_iter : 1\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 7\n","\t\tfinal state to target state shortest dist: 7\n","\n","\t\tnum_iter : 2\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 7\n","\t\tfinal state to target state shortest dist: 7\n","\n","\t\tnum_iter : 3\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 4\n","\t\tfinal state to target state shortest dist: 6\n","\n","\t\tnum_iter : 4\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 7\n","\t\tfinal state to target state shortest dist: 7\n","\n","\taverage episode length = 10002.0\n","\taverage episode reward = -100.02000000001426\n","\n","bathroom_02\n","terminal state id: 37\n","\n","\t\tnum_iter : 0\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 5\n","\t\tfinal state to target state shortest dist: 5\n","\n","\t\tnum_iter : 1\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 9\n","\t\tfinal state to target state shortest dist: 9\n","\n","\t\tnum_iter : 2\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 8\n","\t\tfinal state to target state shortest dist: 8\n","\n","\t\tnum_iter : 3\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 10\n","\t\tfinal state to target state shortest dist: 10\n","\n","\t\tnum_iter : 4\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 4\n","\t\tfinal state to target state shortest dist: 4\n","\n","\taverage episode length = 10002.0\n","\taverage episode reward = -100.02000000001426\n","\n","bathroom_02\n","terminal state id: 43\n","\n","\t\tnum_iter : 0\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 7\n","\t\tfinal state to target state shortest dist: 7\n","\n","\t\tnum_iter : 1\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 3\n","\t\tfinal state to target state shortest dist: 3\n","\n","\t\tnum_iter : 2\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 11\n","\t\tfinal state to target state shortest dist: 11\n","\n","\t\tnum_iter : 3\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 5\n","\t\tfinal state to target state shortest dist: 5\n","\n","\t\tnum_iter : 4\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 8\n","\t\tfinal state to target state shortest dist: 8\n","\n","\taverage episode length = 10002.0\n","\taverage episode reward = -100.02000000001426\n","\n","bathroom_02\n","terminal state id: 53\n","\n","\t\tnum_iter : 0\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 6\n","\t\tfinal state to target state shortest dist: 6\n","\n","\t\tnum_iter : 1\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 3\n","\t\tfinal state to target state shortest dist: 3\n","\n","\t\tnum_iter : 2\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 7\n","\t\tfinal state to target state shortest dist: 7\n","\n","\t\tnum_iter : 3\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 2\n","\t\tfinal state to target state shortest dist: 2\n","\n","\t\tnum_iter : 4\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 9\n","\t\tfinal state to target state shortest dist: 9\n","\n","\taverage episode length = 10002.0\n","\taverage episode reward = -100.02000000001426\n","\n","bathroom_02\n","terminal state id: 69\n","\n","\t\tnum_iter : 0\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 5\n","\t\tfinal state to target state shortest dist: 5\n","\n","\t\tnum_iter : 1\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 4\n","\t\tfinal state to target state shortest dist: 4\n","\n","\t\tnum_iter : 2\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 3\n","\t\tfinal state to target state shortest dist: 3\n","\n","\t\tnum_iter : 3\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 7\n","\t\tfinal state to target state shortest dist: 7\n","\n","\t\tnum_iter : 4\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 8\n","\t\tfinal state to target state shortest dist: 8\n","\n","\taverage episode length = 10002.0\n","\taverage episode reward = -100.02000000001426\n","living_room_08\n","\n","living_room_08\n","terminal state id: 92\n","\n","\t\tnum_iter : 0\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 8\n","\t\tfinal state to target state shortest dist: 8\n","\n","\t\tnum_iter : 1\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 13\n","\t\tfinal state to target state shortest dist: 13\n","\n","\t\tnum_iter : 2\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 7\n","\t\tfinal state to target state shortest dist: 7\n","\n","\t\tnum_iter : 3\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 3\n","\t\tfinal state to target state shortest dist: 3\n","\n","\t\tnum_iter : 4\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 25\n","\t\tfinal state to target state shortest dist: 25\n","\n","\taverage episode length = 10002.0\n","\taverage episode reward = -100.02000000001426\n","\n","living_room_08\n","terminal state id: 135\n","\n","\t\tnum_iter : 0\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 8\n","\t\tfinal state to target state shortest dist: 8\n","\n","\t\tnum_iter : 1\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 8\n","\t\tfinal state to target state shortest dist: 8\n","\n","\t\tnum_iter : 2\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 9\n","\t\tfinal state to target state shortest dist: 9\n","\n","\t\tnum_iter : 3\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 2\n","\t\tfinal state to target state shortest dist: 2\n","\n","\t\tnum_iter : 4\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 4\n","\t\tfinal state to target state shortest dist: 4\n","\n","\taverage episode length = 10002.0\n","\taverage episode reward = -100.02000000001426\n","\n","living_room_08\n","terminal state id: 193\n","\n","\t\tnum_iter : 0\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 23\n","\t\tfinal state to target state shortest dist: 23\n","\n","\t\tnum_iter : 1\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 19\n","\t\tfinal state to target state shortest dist: 19\n","\n","\t\tnum_iter : 2\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 20\n","\t\tfinal state to target state shortest dist: 20\n","\n","\t\tnum_iter : 3\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 30\n","\t\tfinal state to target state shortest dist: 30\n","\n","\t\tnum_iter : 4\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 9\n","\t\tfinal state to target state shortest dist: 9\n","\n","\taverage episode length = 10002.0\n","\taverage episode reward = -100.02000000001426\n","\n","living_room_08\n","terminal state id: 228\n","\n","\t\tnum_iter : 0\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 15\n","\t\tfinal state to target state shortest dist: 15\n","\n","\t\tnum_iter : 1\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 8\n","\t\tfinal state to target state shortest dist: 8\n","\n","\t\tnum_iter : 2\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 12\n","\t\tfinal state to target state shortest dist: 12\n","\n","\t\tnum_iter : 3\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 12\n","\t\tfinal state to target state shortest dist: 12\n","\n","\t\tnum_iter : 4\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 11\n","\t\tfinal state to target state shortest dist: 11\n","\n","\taverage episode length = 10002.0\n","\taverage episode reward = -100.02000000001426\n","\n","living_room_08\n","terminal state id: 254\n","\n","\t\tnum_iter : 0\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 11\n","\t\tfinal state to target state shortest dist: 11\n","\n","\t\tnum_iter : 1\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 3\n","\t\tfinal state to target state shortest dist: 3\n","\n","\t\tnum_iter : 2\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 11\n","\t\tfinal state to target state shortest dist: 11\n","\n","\t\tnum_iter : 3\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 14\n","\t\tfinal state to target state shortest dist: 14\n","\n","\t\tnum_iter : 4\n","\t\tepisode_length : 10002\n","\t\tepisode_reward : -100.02000000001426\n","\t\tinital state to target state shortest dist: 7\n","\t\tfinal state to target state shortest dist: 7\n","\n","\taverage episode length = 10002.0\n","\taverage episode reward = -100.02000000001426\n","\n","scene name:kitchen_02\n","average scene length = 10002.0\n","average scene reward = -100.02000000001426\n","\n","\n","scene name:bathroom_02\n","average scene length = 10002.0\n","average scene reward = -100.02000000001426\n","\n","\n","scene name:living_room_08\n","average scene length = 10002.0\n","average scene reward = -100.02000000001426\n","\n","time: 18min 18s (started: 2022-05-05 05:00:48 +00:00)\n"]}],"source":["# for differenct scenes generalization\n","\n","model_load.to(device_name)\n","model_load.eval()\n","\n","avg_len_scene_dict = {}\n","avg_reward_scene_dict = {}\n","\n","for s in other_scenes:\n","    target_list = TASK_LIST[s]\n","    print(s)\n","\n","    avg_len_scene_dict[s] = []\n","    avg_reward_scene_dict[s] = []\n","\n","    total_episode_len_scene_tr = []\n","    total_episode_reward_scene_tr = []\n","\n","\n","    for target_s in target_list:\n","\n","        target_s_int = int(target_s)\n","\n","        \n","\n","        eval_env = THORDiscreteEnvironment({\n","        'random_start': True,\n","        'scene_name': s,\n","        'h5_file_path': '%s.h5'%s,\n","        'terminal_state_id' : target_s_int\n","        })\n","\n","        print()\n","        print('%s'%s)\n","        print(f'terminal state id: {eval_env.terminal_state_id}')\n","\n","        total_episode_len_eps_tr = np.zeros(NUM_EVAL_EPISODES)\n","        total_episode_reward_eps_tr = np.zeros(NUM_EVAL_EPISODES)\n","\n","        for episode in range(NUM_EVAL_EPISODES):\n","\n","\n","            model_load.eval()\n","            eval_env.reset()\n","\n","            actions = []\n","\n","            final_state = None\n","            inital_state_id = eval_env.current_state_id\n","\n","            terminal_reached = False\n","\n","            episode_length = 0\n","            episode_reward = 0\n","            local_t = 0\n","\n","            terminal = False\n","            inital_state_to_target_dist = eval_env.shortest_path_distances[inital_state_id][eval_env.terminal_state_id]\n","            \n","\n","            while not terminal:\n","\n","                torch_s_t = torch.from_numpy(eval_env.s_t).float().flatten()  # state embedding in torch convertinng from 2048 x 4, to 8192 x 1\n","                torch_target = torch.from_numpy(eval_env.target).float().flatten()  # target embedding in torch\n","\n","                torch_s_t = torch_s_t.to(device_name)\n","                torch_target = torch_target.to(device_name)\n","\n","                actions_prob, value = model_load(torch_s_t, torch_target)\n","\n","                action = torch.argmax(actions_prob)  # choose best action\n","\n","                # store required items\n","                actions.append(action)\n","\n","                # process game\n","                eval_env.step(action)\n","\n","                # receive game result\n","                reward = eval_env.reward\n","                terminal = eval_env.terminal\n","\n","                # ad-hoc reward for navigation\n","                reward = 10.0 if terminal else -0.01\n","                if episode_length > 1e4: terminal = True\n","\n","                episode_reward += reward\n","                episode_length += 1\n","                # episode_max_q = max(episode_max_q, np.max(value_)) !!!! DO Q VALUE CLIPPING LATER !!!!!!!\n","\n","                local_t += 1\n","\n","                # s_t1 -> s_t\n","                eval_env.update()\n","\n","                if terminal:\n","                    final_state = eval_env.s_t[0]\n","                    terminal_end = True\n","                    break\n","\n","            \n","            final_state_to_target_dist = eval_env.shortest_path_distances[eval_env.current_state_id][eval_env.terminal_state_id]\n","            print(f'\\n\\t\\tnum_iter : {episode}\\n\\t\\tepisode_length : {episode_length}')\n","            print(f'\\t\\tepisode_reward : {episode_reward}')\n","            print(f'\\t\\tinital state to target state shortest dist: {inital_state_to_target_dist}')\n","            # print(f'shortest dist from episode termination state to goal state: {dist}')\n","            print(f'\\t\\tfinal state to target state shortest dist: {final_state_to_target_dist}')\n","            \n","            total_episode_len_eps_tr[episode] = episode_length\n","            total_episode_reward_eps_tr[episode] = episode_reward\n","\n","            \n","        print()\n","        print(f'\\taverage episode length = {total_episode_len_eps_tr.mean()}')\n","        print(f'\\taverage episode reward = {total_episode_reward_eps_tr.mean()}')\n","        total_episode_len_scene_tr.append(total_episode_len_eps_tr.mean().item())\n","        total_episode_reward_scene_tr.append(total_episode_reward_eps_tr.mean().item())\n","\n","    avg_len_scene_dict[s].append(total_episode_len_eps_tr.mean())\n","    avg_reward_scene_dict[s].append(total_episode_reward_eps_tr.mean())\n","\n","\n","for s in avg_len_scene_dict.keys():\n","    print()\n","    print(f'scene name:{s}')\n","    print(f'average scene length = {np.array(avg_len_scene_dict[s]).mean()}')\n","    print(f'average scene reward = {np.array(avg_reward_scene_dict[s]).mean()}')\n","    print()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"750K_frames_trained_model_performance_evaluation.ipynb","provenance":[{"file_id":"1OkzIIEB5mUhowVp6SYhLgywF8rZ-GDZK","timestamp":1651345319814}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}