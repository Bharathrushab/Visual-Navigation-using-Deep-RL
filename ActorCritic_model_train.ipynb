{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ActorCritic_model_train.ipynb","provenance":[],"collapsed_sections":["g8GnQBTkHMkq","LNx2VBFCEtiE","duFR7qAxEzvT"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xMYMvgbZDClz","executionInfo":{"status":"ok","timestamp":1651600173226,"user_tz":240,"elapsed":9491,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}},"outputId":"75e9a433-0b94-4dfb-f1d6-87be5fd36282"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 3.72 ms (started: 2022-05-03 17:49:31 +00:00)\n"]}],"source":["%%capture\n","!pip install ipython-autotime\n","\n","%load_ext autotime"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XuTgLgoSku-g","executionInfo":{"status":"ok","timestamp":1651600189816,"user_tz":240,"elapsed":16610,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}},"outputId":"df96277d-b0a2-4dc8-a247-c27456226bfc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","time: 16.5 s (started: 2022-05-03 17:49:31 +00:00)\n"]}]},{"cell_type":"code","source":["# !mkdir 'Data'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UlSS1HEMkyfr","executionInfo":{"status":"ok","timestamp":1651600189818,"user_tz":240,"elapsed":38,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}},"outputId":"bd8279f7-9181-46d6-f990-61be8266eceb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.21 ms (started: 2022-05-03 17:49:48 +00:00)\n"]}]},{"cell_type":"code","source":["!cp -r '/content/gdrive/MyDrive/ESE_650_Project/data/' '/content/'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0PsH3UnqkzID","executionInfo":{"status":"ok","timestamp":1651600193631,"user_tz":240,"elapsed":3836,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}},"outputId":"a18f60ad-3df5-4b7c-f0c6-4bd06706bf3b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 3.64 s (started: 2022-05-03 17:49:48 +00:00)\n"]}]},{"cell_type":"code","source":["!rm -r '/content/Data'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wh0CU0RLl4ot","executionInfo":{"status":"ok","timestamp":1651600193633,"user_tz":240,"elapsed":75,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}},"outputId":"39473669-6818-4f8b-f0b6-d419fa3b86f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["rm: cannot remove '/content/Data': No such file or directory\n","time: 160 ms (started: 2022-05-03 17:49:51 +00:00)\n"]}]},{"cell_type":"code","source":["# !pip3 uninstall tensorboard\n","# !pip3 uninstall tensorflow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M8-is1EcO12x","executionInfo":{"status":"ok","timestamp":1651600193635,"user_tz":240,"elapsed":45,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}},"outputId":"0eb375c5-7fa2-4bac-adcc-16a83bee8d1a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.28 ms (started: 2022-05-03 17:49:52 +00:00)\n"]}]},{"cell_type":"code","source":["# !pip uninstall -y tensorboard-plugin-wit"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1zd0hRB2LtV0","executionInfo":{"status":"ok","timestamp":1651600193636,"user_tz":240,"elapsed":31,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}},"outputId":"620f9ede-0c01-4650-c81a-6b1380da0384"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 3.2 ms (started: 2022-05-03 17:49:52 +00:00)\n"]}]},{"cell_type":"code","source":["# !pip3 install --ignore-installed tf-nightly"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t5wpVvqnO3Rp","executionInfo":{"status":"ok","timestamp":1651600193760,"user_tz":240,"elapsed":140,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}},"outputId":"62bcedc9-45f7-4eea-a242-9944921c505c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 805 Âµs (started: 2022-05-03 17:49:52 +00:00)\n"]}]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","\n","from tqdm import tqdm\n","\n","import matplotlib.pyplot as plt\n","\n","# logging\n","from torch.utils.tensorboard import SummaryWriter\n","# import wandb\n","# from wandb.fastai import WandbCallback\n","import pandas as pd\n","\n","## scene loader \n","import sys\n","import h5py\n","import json\n","import numpy as np\n","import random\n","import skimage.io\n","from skimage.transform import resize\n","\n","import os"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C4O7BDpgk9wv","executionInfo":{"status":"ok","timestamp":1651600199913,"user_tz":240,"elapsed":6159,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}},"outputId":"84f232fa-62e3-42f8-92ca-5367f094c3ca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 6.14 s (started: 2022-05-03 17:49:52 +00:00)\n"]}]},{"cell_type":"markdown","source":["# Seeding for reproducability"],"metadata":{"id":"g8GnQBTkHMkq"}},{"cell_type":"code","source":["def seed_everything(seed=42):\n","  random.seed(seed)\n","  os.environ['PYTHONHASHSEED'] = str(seed)\n","  np.random.seed(seed)\n","  torch.manual_seed(seed)\n","  torch.backends.cudnn.deterministic = True\n","  torch.backends.cudnn.benchmark = False\n","  np.random.seed(seed)\n","\n","  return\n","\n","seed_everything()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SSArD4e_zjRt","executionInfo":{"status":"ok","timestamp":1651600199917,"user_tz":240,"elapsed":47,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}},"outputId":"4bcb551a-2673-4fd1-e40b-750aa3ec3598"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 22.3 ms (started: 2022-05-03 17:49:58 +00:00)\n"]}]},{"cell_type":"markdown","source":["# Hyperparameters"],"metadata":{"id":"LNx2VBFCEtiE"}},{"cell_type":"code","source":["## Constants.py\n","\n","# -*- coding: utf-8 -*-\n","\n","LOCAL_T_MAX = 5 # repeat step size\n","RMSP_ALPHA = 0.99 # decay parameter for RMSProp\n","RMSP_EPSILON = 0.1 # epsilon parameter for RMSProp\n","LEARNING_RATE = 1e-2\n","GAMMA = 0.99 # discount factor for rewards\n","ENTROPY_BETA = 0.01 # entropy regurarlization constant\n","\n","SCREEN_WIDTH = 84\n","SCREEN_HEIGHT = 84\n","HISTORY_LENGTH = 4\n","\n","ACTION_SIZE=4\n","\n","# keys are scene names, and values are a list of location ids (navigation targets)\n","TASK_LIST = {\n","  'bathroom_02'    : ['26', '37', '43', '53', '69'],\n","  'bedroom_04'     : ['134', '264', '320', '384', '387'],\n","  'kitchen_02'     : ['90', '136', '157', '207', '329'],\n","  'living_room_08' : ['92', '135', '193', '228', '254']\n","}\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c8MS6DQBmvGW","executionInfo":{"status":"ok","timestamp":1651600199922,"user_tz":240,"elapsed":38,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}},"outputId":"9ea8eabf-e5ba-4577-eebb-3d8279754e79"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 29.3 ms (started: 2022-05-03 17:49:58 +00:00)\n"]}]},{"cell_type":"markdown","source":["# Simulation environment setup"],"metadata":{"id":"duFR7qAxEzvT"}},{"cell_type":"code","source":["## Scene Loader\n","\n","# -*- coding: utf-8 -*-\n","class THORDiscreteEnvironment(object):\n","\n","  def __init__(self, config=dict()):\n","\n","    # configurations\n","    self.scene_name          = config.get('scene_name', 'bedroom_04')\n","    self.random_start        = config.get('random_start', True)\n","    self.n_feat_per_locaiton = config.get('n_feat_per_locaiton', 1) # 1 for no sampling\n","    self.terminal_state_id   = config.get('terminal_state_id', 0)\n","\n","    self.h5_file_path = config.get('h5_file_path', 'data/%s.h5'%self.scene_name)\n","    self.h5_file      = h5py.File(self.h5_file_path, 'r')\n","\n","    self.locations   = self.h5_file['location'][()]\n","    self.rotations   = self.h5_file['rotation'][()]\n","    self.n_locations = self.locations.shape[0]\n","\n","    self.terminals = np.zeros(self.n_locations)\n","    self.terminals[self.terminal_state_id] = 1\n","    self.terminal_states, = np.where(self.terminals)\n","\n","    self.transition_graph = self.h5_file['graph'][()]\n","    self.shortest_path_distances = self.h5_file['shortest_path_distance'][()]\n","\n","    self.history_length = HISTORY_LENGTH\n","    self.screen_height  = SCREEN_HEIGHT\n","    self.screen_width   = SCREEN_WIDTH\n","\n","    # we use pre-computed fc7 features from ResNet-50\n","    # self.s_t = np.zeros([self.screen_height, self.screen_width, self.history_length])\n","    self.s_t      = np.zeros([2048, self.history_length])\n","    self.s_t1     = np.zeros_like(self.s_t)\n","    self.s_target = self._tiled_state(self.terminal_state_id)\n","\n","    self.reset()\n","\n","  # public methods\n","\n","  def reset(self):\n","    # randomize initial state\n","    while True:\n","      k = random.randrange(self.n_locations)\n","      min_d = np.inf\n","      # check if target is reachable\n","      for t_state in self.terminal_states:\n","        dist = self.shortest_path_distances[k][t_state]\n","        min_d = min(min_d, dist)\n","      # min_d = 0  if k is a terminal state\n","      # min_d = -1 if no terminal state is reachable from k\n","      if min_d > 0: break\n","\n","    # reset parameters\n","    self.current_state_id = k\n","    self.s_t = self._tiled_state(self.current_state_id)\n","\n","    self.reward   = 0\n","    self.collided = False\n","    self.terminal = False\n","\n","  def step(self, action):\n","    assert not self.terminal, 'step() called in terminal state'\n","    k = self.current_state_id\n","    if self.transition_graph[k][action] != -1:\n","      self.current_state_id = self.transition_graph[k][action]\n","      if self.terminals[self.current_state_id]:\n","        self.terminal = True\n","        self.collided = False\n","      else:\n","        self.terminal = False\n","        self.collided = False\n","    else:\n","      self.terminal = False\n","      self.collided = True\n","\n","    self.reward = self._reward(self.terminal, self.collided)\n","    self.s_t1 = np.append(self.s_t[:,1:], self.state, axis=1)\n","\n","  def update(self):\n","    self.s_t = self.s_t1\n","\n","  # private methods\n","\n","  def _tiled_state(self, state_id):\n","    k = random.randrange(self.n_feat_per_locaiton)\n","    f = self.h5_file['resnet_feature'][state_id][k][:,np.newaxis]\n","    return np.tile(f, (1, self.history_length))\n","\n","  def _reward(self, terminal, collided):\n","    # positive reward upon task completion\n","    if terminal: return 10.0\n","    # time penalty or collision penalty\n","    return -0.1 if collided else -0.01\n","\n","  # properties\n","\n","  @property\n","  def action_size(self):\n","    # move forward/backward, turn left/right for navigation\n","    return ACTION_SIZE \n","\n","  @property\n","  def action_definitions(self):\n","    action_vocab = [\"MoveForward\", \"RotateRight\", \"RotateLeft\", \"MoveBackward\"]\n","    return action_vocab[:ACTION_SIZE]\n","\n","  @property\n","  def observation(self):\n","    return self.h5_file['observation'][self.current_state_id]\n","\n","  @property\n","  def state(self):\n","    # read from hdf5 cache\n","    k = random.randrange(self.n_feat_per_locaiton)\n","    return self.h5_file['resnet_feature'][self.current_state_id][k][:,np.newaxis]\n","\n","  @property\n","  def target(self):\n","    return self.s_target\n","\n","  @property\n","  def x(self):\n","    return self.locations[self.current_state_id][0]\n","\n","  @property\n","  def z(self):\n","    return self.locations[self.current_state_id][1]\n","\n","  @property\n","  def r(self):\n","    return self.rotations[self.current_state_id]\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J3a0P56lmyuo","executionInfo":{"status":"ok","timestamp":1651600200363,"user_tz":240,"elapsed":472,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}},"outputId":"d3dde27e-574a-4422-e135-cb93928ac09f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 438 ms (started: 2022-05-03 17:49:58 +00:00)\n"]}]},{"cell_type":"code","source":["## load scene into environment\n","scene_name = 'bedroom_04'\n","\n","env = THORDiscreteEnvironment({\n","'random_start': True,\n","'scene_name': scene_name,\n","'h5_file_path': 'data/%s.h5'%scene_name,\n","'terminal_state_id' : 134\n","})\n","\n","print(env.scene_name)\n","print(env.terminal_state_id)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7bCARfC4oCO9","executionInfo":{"status":"ok","timestamp":1651600200368,"user_tz":240,"elapsed":30,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}},"outputId":"e711065c-0ea9-4fcd-abe1-8c67bfca0b6f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["bedroom_04\n","134\n","time: 29 ms (started: 2022-05-03 17:49:58 +00:00)\n"]}]},{"cell_type":"markdown","source":["# Logging Setup"],"metadata":{"id":"UG0VN46jE6Hd"}},{"cell_type":"code","source":["# train_args = {\n","#     'save_eval_checkpoints': False,\n","#     'learning_rate':1e-2, \n","#     'num_train_epochs': 100, \n","#     'reprocess_input_data': True, \n","#     'overwrite_output_dir': True,\n","#     'wandb_project': \"ese_650_ac_try_1\",\n","# }\n","\n","#     # 'num_train_epochs': 3, \n","\n","\n","# # train_args = {\n","# #     'evaluate_during_training': True,\n","# #     'logging_steps': 10,\n","# #     'evaluate_during_training_steps': 10,\n","# #     'save_eval_checkpoints': False,\n","# #     'train_batch_size': 128,\n","# #     'eval_batch_size': 256,\n","# #     'learning_rate':1e-5, \n","# #     'num_train_epochs': 1, \n","# #     'reprocess_input_data': True, \n","# #     'overwrite_output_dir': True,\n","# #     'wandb_project': \"visualization-demo\",\n","# # }"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wzj0s3Va8R3F","executionInfo":{"status":"ok","timestamp":1651600200371,"user_tz":240,"elapsed":24,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}},"outputId":"80fd86b9-28f6-4275-bd1c-0c5988245a13"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 17.1 ms (started: 2022-05-03 17:49:58 +00:00)\n"]}]},{"cell_type":"code","source":["# wandb.init(anonymous=\"allow\", config=train_args, entity = 'AC model')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"odsGvMmX8Ozi","executionInfo":{"status":"ok","timestamp":1651600200526,"user_tz":240,"elapsed":174,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}},"outputId":"80b2ff7e-50ba-433e-e1fb-554d050ab76c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 861 Âµs (started: 2022-05-03 17:49:59 +00:00)\n"]}]},{"cell_type":"code","source":["!rm -rf ./runs/ "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p147QzTlNYeK","executionInfo":{"status":"ok","timestamp":1651600200766,"user_tz":240,"elapsed":245,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}},"outputId":"afb53f70-b9c7-4df3-c55b-a76ea31753f9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 174 ms (started: 2022-05-03 17:49:59 +00:00)\n"]}]},{"cell_type":"code","source":["!mkdir 'runs'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fNrnHKO8FGck","executionInfo":{"status":"ok","timestamp":1651600200924,"user_tz":240,"elapsed":163,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}},"outputId":"a3ac1f9f-3535-4b19-ea46-651c401e2523"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 154 ms (started: 2022-05-03 17:49:59 +00:00)\n"]}]},{"cell_type":"code","source":["# !mkdir './runs/AC_model'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gi05WCoTFNlK","executionInfo":{"status":"ok","timestamp":1651600200925,"user_tz":240,"elapsed":15,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}},"outputId":"8a6b2ca8-65cb-40f9-daff-6365786f6521"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.12 ms (started: 2022-05-03 17:49:59 +00:00)\n"]}]},{"cell_type":"code","source":["# writer = SummaryWriter('./runs/AC_model')\n","writer = SummaryWriter()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zQsOCzctEpsW","executionInfo":{"status":"ok","timestamp":1651600205908,"user_tz":240,"elapsed":4990,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}},"outputId":"4ae8c36a-f251-4fef-f9d4-3eef95a374ef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 4.95 s (started: 2022-05-03 17:49:59 +00:00)\n"]}]},{"cell_type":"code","source":["d = {\"epoch\":[], \"train_episode_reward\":[], \"train_episode_length\":[], \"train_value_loss\":[], \"train_policy_loss\":[], \"train_total_loss\":[]}\n","log_df = pd.DataFrame(d)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JwNDuf3_d4-u","executionInfo":{"status":"ok","timestamp":1651600205912,"user_tz":240,"elapsed":55,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}},"outputId":"92f3bf0a-8ab9-4e35-87e4-39f9b180e533"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 14.7 ms (started: 2022-05-03 17:50:04 +00:00)\n"]}]},{"cell_type":"code","source":["log_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67},"id":"Xsle35A1ep19","executionInfo":{"status":"ok","timestamp":1651600205916,"user_tz":240,"elapsed":49,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}},"outputId":"5ca30aeb-f2cc-4a9d-80b6-4e63cf1b3122"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Empty DataFrame\n","Columns: [epoch, train_episode_reward, train_episode_length, train_value_loss, train_policy_loss, train_total_loss]\n","Index: []"],"text/html":["\n","  <div id=\"df-591494d2-4eed-4e79-a6d8-fbd23348bdd6\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>epoch</th>\n","      <th>train_episode_reward</th>\n","      <th>train_episode_length</th>\n","      <th>train_value_loss</th>\n","      <th>train_policy_loss</th>\n","      <th>train_total_loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-591494d2-4eed-4e79-a6d8-fbd23348bdd6')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-591494d2-4eed-4e79-a6d8-fbd23348bdd6 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-591494d2-4eed-4e79-a6d8-fbd23348bdd6');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":21},{"output_type":"stream","name":"stdout","text":["time: 33 ms (started: 2022-05-03 17:50:04 +00:00)\n"]}]},{"cell_type":"markdown","source":["# Actor-Critic Model"],"metadata":{"id":"2d3b6GdQHZSx"}},{"cell_type":"code","source":["class MyActorCritic(nn.Module):\n","\n","    def __init__(self):\n","        super(MyActorCritic, self).__init__()\n","\n","        # generic siamese layer: <start>\n","        self.fc1 = nn.Linear(in_features=8192, out_features=512)\n","        self.fc2 = nn.Linear(in_features=1024, out_features=512)\n","        # generic siamese layer: <end>\n","\n","        # Scene specific layer: <start> \n","        # for now we have only one scene speicific layer and only one thread i.e only one target in one scene\n","        self.fc3 = nn.Linear(in_features=512, out_features=512)\n","\n","        self.actions_fc = nn.Linear(in_features=512, out_features=4)\n","        self.value_fc  = nn.Linear(in_features=512, out_features=1)\n","        # Scene specific layer: <end>\n","\n","\n","    def forward(self, input_image_embedding, target_image_embedding):\n","\n","        # generic siamese latyer\n","        x1 = F.relu(self.fc1(input_image_embedding))\n","        x2 = F.relu(self.fc1(target_image_embedding))\n","\n","        x_combined = torch.cat((x1, x2)) # combine input image emb and target image emb, may need to add axis for concat later\n","\n","        x = F.relu(self.fc2(x_combined))\n","\n","\n","        # scene specific layer\n","\n","        x = F.relu(self.fc3(x))\n","        x1 = self.actions_fc(x)\n","        # print(f' actions values before softmax:\\n{x1}\\n')\n","        actions_prob = F.softmax(x1, dim=0)\n","        value = self.value_fc(x)\n","\n","        return actions_prob, value\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6yAh2zBytgFl","executionInfo":{"status":"ok","timestamp":1651600206057,"user_tz":240,"elapsed":179,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}},"outputId":"38d11bba-a041-4819-f116-5a8746a37433"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 58.1 ms (started: 2022-05-03 17:50:04 +00:00)\n"]}]},{"cell_type":"code","source":["num_epochs = 50000\n","model = MyActorCritic()\n","lr = LEARNING_RATE\n","optimizer = torch.optim.RMSprop(model.parameters(), lr=lr, alpha=RMSP_ALPHA, eps=RMSP_EPSILON)\n","device_name = 'cuda:0'\n","# device_name = 'cpu'\n","\n","max_local_timestep = LOCAL_T_MAX\n","entropy_beta = ENTROPY_BETA\n","\n","torch.autograd.set_detect_anomaly(True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"18ySNjFJ0inq","executionInfo":{"status":"ok","timestamp":1651600206269,"user_tz":240,"elapsed":221,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}},"outputId":"9cd215b2-acf8-46c3-da59-f76e7543db9e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7fb590053f90>"]},"metadata":{},"execution_count":23},{"output_type":"stream","name":"stdout","text":["time: 163 ms (started: 2022-05-03 17:50:04 +00:00)\n"]}]},{"cell_type":"code","source":["model.to(device_name)\n","\n","for epoch in tqdm(range(num_epochs)):\n","\n","    epoch_loss = 0\n","    # model.double()\n","    # model.train()\n","\n","    env.reset()\n","\n","    states = []\n","    actions = []\n","    policy = []\n","    rewards = []\n","    values = []\n","    targets = []\n","\n","    terminal_reached = False\n","\n","    episode_length = 0\n","    episode_reward = 0\n","    local_t = 0\n","\n","    for t in range(max_local_timestep):\n","\n","        torch_s_t = torch.from_numpy(env.s_t).float().flatten()  # state embedding in torch convertinng from 2048 x 4, to 8192 x 1\n","        torch_target = torch.from_numpy(env.target).float().flatten()  # target embedding in torch\n","\n","        torch_s_t = torch_s_t.to(device_name)\n","        torch_target = torch_target.to(device_name)\n","\n","        actions_prob, value = model(torch_s_t, torch_target)\n","\n","        action = None\n","        with torch.no_grad():\n","            action = torch.multinomial(actions_prob, 1).item()  # sample action according to the probability returned by the network\n","\n","\n","        # store required items\n","        states.append(env.s_t)\n","        policy.append(actions_prob)\n","        actions.append(action)\n","        values.append(value)\n","        targets.append(env.target)\n","\n","        # process game\n","        env.step(action)\n","\n","        # receive game result\n","        reward = env.reward\n","        terminal = env.terminal\n","\n","        # ad-hoc reward for navigation\n","        reward = 10.0 if terminal else -0.01\n","        if episode_length > 5e3: terminal = True\n","\n","        episode_reward += reward\n","        episode_length += 1\n","        # episode_max_q = max(episode_max_q, np.max(value_)) !!!! DO Q VALUE CLIPPING LATER !!!!!!!\n","\n","        # clip and append reward\n","        rewards.append(np.clip(reward, -1, 1))\n","\n","        local_t += 1\n","\n","        # s_t1 -> s_t\n","        env.update()\n","\n","        if terminal:\n","            terminal_end = True\n","            # sys.stdout.write(\"time %d | thread #%d | scene %s | target #%s\\n%s %s episode reward = %.3f\\n%s %s episode length = %d\\n%s %s episode max Q  = %.3f\\n\" % (global_t, self.thread_index, self.scene_scope, self.task_scope, self.scene_scope, self.task_scope, self.episode_reward, self.scene_scope, self.task_scope, self.episode_length, self.scene_scope, self.task_scope, self.episode_max_q))\n","\n","            # summary_values = {\n","            # \"episode_reward_input\": episode_reward,\n","            # \"episode_length_input\": float(episode_length),\n","            # # \"episode_max_q_input\": episode_max_q,\n","            # # \"learning_rate_input\": _anneal_learning_rate(global_t)\n","            # }\n","\n","            # self._record_score(sess, summary_writer, summary_op, summary_placeholders,\n","            #                 summary_values, global_t)\n","            # self.episode_reward = 0\n","            # self.episode_length = 0\n","            # self.episode_max_q = -np.inf\n","            # self.env.reset()\n","            # TODO: add logging\n","            # print('playout finished')\n","            # print(f'episode length: {episode_length}')\n","            # print(f'episode reward: {episode_reward}')\n","            # print(f'episode max_q: {episode_max_q}')\n","\n","\n","            break\n","\n","    writer.add_scalar(\"train/episode_reward\", episode_reward, epoch)\n","    writer.add_scalar(\"train/episode_length\", episode_length, epoch)\n","\n","    torch_s_t = torch.from_numpy(env.s_t).float().flatten()  # state embedding in torch\n","    torch_target = torch.from_numpy(env.target).float().flatten()  # target embedding in torch\n","\n","    torch_s_t = torch_s_t.to(device_name)\n","    torch_target = torch_target.to(device_name)\n","\n","    next_state_value = 0.0\n","    if not terminal:\n","        _, next_state_value = model(torch_s_t, torch_target)\n","        next_state_value = next_state_value.item()\n","\n","    actions.reverse()\n","    states.reverse()\n","    rewards.reverse()\n","    values.reverse()\n","    policy.reverse()\n","\n","    batch_si = []\n","    batch_policy = []\n","    batch_a = []\n","    batch_td = []\n","    batch_R = []\n","    batch_t = []\n","    batch_values = []\n","\n","    # compute and accmulate gradients\n","    for (pi, ai, ri, si, Vi, ti) in zip(policy, actions, rewards, states, values, targets):\n","        \n","        next_state_value = ri + GAMMA * next_state_value\n","        td = next_state_value - Vi\n","        a = np.zeros([ACTION_SIZE])\n","        a[ai] = 1\n","\n","        batch_si.append(si)\n","        batch_policy.append(pi)\n","        batch_a.append(a)\n","        batch_td.append(td)\n","        batch_R.append(next_state_value)\n","        batch_t.append(ti)\n","        batch_values.append(Vi)\n","\n","    # COMPUTE LOSS + BACKPROP\n","    # !!!!!!!! compute actor/policy loss !!!!!!!!!!!!\n","    batch_policy_tensor = torch.stack(batch_policy, 0)\n","\n","    batch_policy_tensor = torch.clip(batch_policy_tensor, 1e-15, 1.0)\n","    policy_log_prob = torch.log(batch_policy_tensor)\n","    policy_entropy = - torch.sum(batch_policy_tensor * policy_log_prob, axis=1)\n","\n","\n","    batch_a_tensor = torch.from_numpy(np.array(batch_a, dtype=np.int64)).to(device_name)\n","    nllLoss = F.nll_loss(policy_log_prob, torch.argmax(batch_a_tensor, dim=1), reduce=False)\n","\n","    batch_td_tensor = torch.stack(batch_td)\n","    policy_loss = nllLoss * batch_td_tensor - policy_entropy * entropy_beta\n","    policy_loss = policy_loss.sum()\n","\n","    # !!!!!!!!!!!! Value loss !!!!!!!!!!!!!!\n","    # learning rate for critic is half of actor's\n","    # Equivalent to 0.5 * l2 loss\n","    r_tensor = torch.from_numpy(np.array(batch_R, dtype=np.float32)).to(device_name)\n","    value_loss = (0.5 * 0.5) * F.mse_loss(torch.stack(batch_values), r_tensor, size_average=False)\n","    # value_loss = (0.5 * 0.5) * F.mse_loss(torch.stack(batch_values), r_tensor)\n","\n","    total_loss = value_loss + policy_loss\n","\n","    optimizer.zero_grad()\n","    total_loss.backward()\n","    optimizer.step()\n","\n","\n","    writer.add_scalar(\"train/value_loss\", value_loss.cpu().detach().item(), epoch)\n","    writer.add_scalar(\"train/policy_loss\", policy_loss.cpu().detach().item(), epoch)\n","    writer.add_scalar(\"train/total_loss\", total_loss.cpu().detach().item(), epoch)\n","\n","\n","    #                {\"epoch\",\"train_episode_reward\":[], \"train_episode_length\":[], \"train_value_loss\":[], \"train_policy_loss\":[], \"train_total_loss\":[]}\n","    log_df.loc[len(log_df.index)] = [epoch, episode_reward, episode_length, value_loss.cpu().detach().item(), policy_loss.cpu().detach().item(), total_loss.cpu().detach().item()] # add row to df"],"metadata":{"id":"FuMFQzTSxBnd","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4beb3e63-0439-4cd4-f348-3c375f409439"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/50000 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:159: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n","  0%|          | 102/50000 [00:13<1:50:57,  7.50it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:159: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  0%|          | 230/50000 [00:29<1:48:40,  7.63it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:159: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  1%|          | 481/50000 [01:02<2:08:07,  6.44it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:159: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  8%|â         | 3795/50000 [08:09<1:40:38,  7.65it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:159: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"," 82%|âââââââââ | 41004/50000 [1:28:35<20:02,  7.48it/s]"]}]},{"cell_type":"code","source":["writer.flush()\n","writer.close()"],"metadata":{"id":"UmjibOkkGdDH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls"],"metadata":{"id":"bd8Xqchjiv4L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd './runs'"],"metadata":{"id":"HGHMZlEpilLA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pwd"],"metadata":{"id":"-QyTiYVbiqFD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cp -r '.' '/content/gdrive/MyDrive//ESE_650_Project/train_metrics/'"],"metadata":{"id":"D1bc7NrAhDB-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd .."],"metadata":{"id":"QCrriZCZjJcK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save the model parameters\n","import time\n","\n","timestr = time.strftime(\"%d_%m-%H_%M_%S\")\n","save_train_metrics_filename = env.scene_name +'_train_metrics_epoch_' + str(num_epochs) + '_' + timestr  + '.csv'"],"metadata":{"id":"zVroQBBlgf_W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save dataframe\n","\n","log_df.to_csv(save_train_metrics_filename)"],"metadata":{"id":"ezqL5OOVgRZc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cp {save_train_metrics_filename} '/content/gdrive/MyDrive//ESE_650_Project/train_metrics/'"],"metadata":{"id":"u4GeOpyxgr_c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Save model"],"metadata":{"id":"1CkyC5ztGpdV"}},{"cell_type":"code","source":["torch.save(model, 'model.pth')"],"metadata":{"id":"tSx1RJdMK6Xx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cp -r 'model.pth' '/content/gdrive/MyDrive/ESE_650_Project/models/'"],"metadata":{"id":"t8bc2r5XJHly"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model = torch.load('model.pth')"],"metadata":{"id":"bcOCBbuZMBqR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# policy_log_prob"],"metadata":{"id":"yjc_u6l90ORh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # print(c_nan)\n","# print(torch.isnan(t_img_embedding).any())"],"metadata":{"id":"RKwzD6sjwGBm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for i, k in enumerate(model.parameters()):\n","#     if i == 2:\n","#         print(k[(torch.logical_not(torch.isnan(k)))].min())\n","#         print()"],"metadata":{"id":"aGp16sWWxdEh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save the model parameters\n","import time\n","\n","timestr = time.strftime(\"%d_%m-%H_%M_%S\")\n","save_model_filename = env.scene_name +'myA2C_model_wts_epoch_' + str(num_epochs) + '_' + timestr  + '.pth'\n","torch.save(model, save_model_filename)"],"metadata":{"id":"k26Pnvf_roEP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cp {save_model_filename} '/content/gdrive/MyDrive//ESE_650_Project/models/'"],"metadata":{"id":"AchqiO1tfV5d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Visualize training results"],"metadata":{"id":"jqMcn4kNGsPj"}},{"cell_type":"code","source":["# %load_ext tensorboard"],"metadata":{"id":"Gi60cR06KShd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# %tensorboard --logdir ./runs"],"metadata":{"id":"-G7iEbcgGyLw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation"],"metadata":{"id":"EWXnJW9j182X"}},{"cell_type":"code","source":["\n","# for episode in tqdm(range(NUM_EVAL_EPISODES)):\n","for episode in tqdm(range(5)):\n","\n","\n","    model.eval()\n","    env.reset()\n","\n","    actions = []\n","\n","    final_state = None\n","    inital_state_id = env.current_state_id\n","\n","    terminal_reached = False\n","\n","    episode_length = 0\n","    episode_reward = 0\n","    local_t = 0\n","\n","    terminal = False\n","    inital_state_to_target_dist = env.shortest_path_distances[inital_state_id][env.terminal_state_id]\n","    \n","\n","    while not terminal:\n","\n","        torch_s_t = torch.from_numpy(env.s_t).float().flatten()  # state embedding in torch convertinng from 2048 x 4, to 8192 x 1\n","        torch_target = torch.from_numpy(env.target).float().flatten()  # target embedding in torch\n","\n","        torch_s_t = torch_s_t.to(device_name)\n","        torch_target = torch_target.to(device_name)\n","\n","        actions_prob, value = model(torch_s_t, torch_target)\n","\n","        action = torch.argmax(actions_prob)  # choose best action\n","\n","        # store required items\n","        actions.append(action)\n","\n","        # process game\n","        env.step(action)\n","\n","        # receive game result\n","        reward = env.reward\n","        terminal = env.terminal\n","\n","        # ad-hoc reward for navigation\n","        reward = 10.0 if terminal else -0.01\n","        if episode_length > 5e3: terminal = True\n","\n","        episode_reward += reward\n","        episode_length += 1\n","        # episode_max_q = max(episode_max_q, np.max(value_)) !!!! DO Q VALUE CLIPPING LATER !!!!!!!\n","\n","        local_t += 1\n","\n","        # s_t1 -> s_t\n","        env.update()\n","\n","        if terminal:\n","            final_state = env.s_t[0]\n","            terminal_end = True\n","            break\n","\n","    \n","    final_state_to_target_dist = env.shortest_path_distances[env.current_state_id][env.terminal_state_id]\n","    print(f'\\nnum_iter : {episode}\\nepisode_length : {episode_length}')\n","    print(f'inital state to target state shortest dist: {inital_state_to_target_dist}')\n","    # print(f'shortest dist from episode termination state to goal state: {dist}')\n","    print(f'final state to target state shortest dist: {final_state_to_target_dist}')\n","    print()\n","\n","    "],"metadata":{"id":"ldOqxD5X2HDO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"VQNHAeSCN8HJ"},"execution_count":null,"outputs":[]}]}