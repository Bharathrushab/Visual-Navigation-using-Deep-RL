{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ESE_650_Project_Notebook_original_pytorch.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xMYMvgbZDClz","executionInfo":{"status":"ok","timestamp":1651343091538,"user_tz":240,"elapsed":5763,"user":{"displayName":"Bharathrushab Manthripragada","userId":"09138793677898015132"}},"outputId":"b5320157-ba54-44d2-e0bd-8455d23859ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 2.57 ms (started: 2022-04-30 18:24:51 +00:00)\n"]}],"source":["%%capture\n","!pip install ipython-autotime\n","\n","%load_ext autotime"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XuTgLgoSku-g","executionInfo":{"status":"ok","timestamp":1651343114891,"user_tz":240,"elapsed":19083,"user":{"displayName":"Bharathrushab Manthripragada","userId":"09138793677898015132"}},"outputId":"6d94aba4-2426-4529-ac3e-4fc3bbb5224f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","time: 18.8 s (started: 2022-04-30 18:24:55 +00:00)\n"]}]},{"cell_type":"code","source":["# !mkdir 'Data'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UlSS1HEMkyfr","executionInfo":{"status":"ok","timestamp":1651302564850,"user_tz":240,"elapsed":13,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}},"outputId":"de766bb3-5d8f-4974-b530-224cbea336a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.59 ms (started: 2022-04-30 07:09:23 +00:00)\n"]}]},{"cell_type":"code","source":["!cp -r '/content/gdrive/MyDrive/ESE_650_Project/data/' '/content/'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0PsH3UnqkzID","executionInfo":{"status":"ok","timestamp":1651343120174,"user_tz":240,"elapsed":2086,"user":{"displayName":"Bharathrushab Manthripragada","userId":"09138793677898015132"}},"outputId":"76d62a2d-4baf-48a5-872a-695802d0313a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.93 s (started: 2022-04-30 18:25:18 +00:00)\n"]}]},{"cell_type":"code","source":["!rm -r '/content/Data'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wh0CU0RLl4ot","executionInfo":{"status":"ok","timestamp":1651343122681,"user_tz":240,"elapsed":155,"user":{"displayName":"Bharathrushab Manthripragada","userId":"09138793677898015132"}},"outputId":"1869697a-9760-4255-f847-363144ef4a4b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["rm: cannot remove '/content/Data': No such file or directory\n","time: 111 ms (started: 2022-04-30 18:25:22 +00:00)\n"]}]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","\n","from tqdm import tqdm\n","\n","import matplotlib.pyplot as plt\n","\n","\n","## scene loader \n","import sys\n","import h5py\n","import json\n","import numpy as np\n","import random\n","import skimage.io\n","from skimage.transform import resize\n","\n","import os"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C4O7BDpgk9wv","executionInfo":{"status":"ok","timestamp":1651343131500,"user_tz":240,"elapsed":3117,"user":{"displayName":"Bharathrushab Manthripragada","userId":"09138793677898015132"}},"outputId":"0309bc9f-eea7-4dd7-dd82-960de5194d46"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 2.95 s (started: 2022-04-30 18:25:28 +00:00)\n"]}]},{"cell_type":"code","source":["def seed_everything(seed=42):\n","  random.seed(seed)\n","  os.environ['PYTHONHASHSEED'] = str(seed)\n","  np.random.seed(seed)\n","  torch.manual_seed(seed)\n","  torch.backends.cudnn.deterministic = True\n","  torch.backends.cudnn.benchmark = False\n","  np.random.seed(seed)\n","\n","  return\n","\n","seed_everything()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SSArD4e_zjRt","executionInfo":{"status":"ok","timestamp":1651343131500,"user_tz":240,"elapsed":12,"user":{"displayName":"Bharathrushab Manthripragada","userId":"09138793677898015132"}},"outputId":"b810899a-4134-4170-99fc-3180e6c8f83a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 12 ms (started: 2022-04-30 18:25:31 +00:00)\n"]}]},{"cell_type":"code","source":["## Constants.py\n","\n","# -*- coding: utf-8 -*-\n","\n","LOCAL_T_MAX = 5 # repeat step size\n","RMSP_ALPHA = 0.99 # decay parameter for RMSProp\n","RMSP_EPSILON = 0.1 # epsilon parameter for RMSProp\n","CHECKPOINT_DIR = 'checkpoints'\n","LOG_FILE = 'logs'\n","INITIAL_ALPHA_LOW = 1e-4    # log_uniform low limit for learning rate\n","INITIAL_ALPHA_HIGH = 1e-2   # log_uniform high limit for learning rate\n","\n","PARALLEL_SIZE = 20 # parallel thread size\n","ACTION_SIZE = 4 # action size\n","\n","INITIAL_ALPHA_LOG_RATE = 0.4226 # log_uniform interpolate rate for learning rate (around 7 * 10^-4)\n","GAMMA = 0.99 # discount factor for rewards\n","ENTROPY_BETA = 0.01 # entropy regurarlization constant\n","MAX_TIME_STEP = 10.0 * 10**6 # 10 million frames\n","GRAD_NORM_CLIP = 40.0 # gradient norm clipping\n","USE_GPU = True # To use GPU, set True\n","VERBOSE = True\n","\n","SCREEN_WIDTH = 84\n","SCREEN_HEIGHT = 84\n","HISTORY_LENGTH = 4\n","\n","NUM_EVAL_EPISODES = 100 # number of episodes for evaluation\n","\n","TASK_TYPE = 'navigation' # no need to change\n","# keys are scene names, and values are a list of location ids (navigation targets)\n","TASK_LIST = {\n","  'bathroom_02'    : ['26', '37', '43', '53', '69'],\n","  'bedroom_04'     : ['134', '264', '320', '384', '387'],\n","  'kitchen_02'     : ['90', '136', '157', '207', '329'],\n","  'living_room_08' : ['92', '135', '193', '228', '254']\n","}\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c8MS6DQBmvGW","executionInfo":{"status":"ok","timestamp":1651343132068,"user_tz":240,"elapsed":8,"user":{"displayName":"Bharathrushab Manthripragada","userId":"09138793677898015132"}},"outputId":"721da46c-a5ae-40e1-ec63-dc313866f76b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 11.2 ms (started: 2022-04-30 18:25:31 +00:00)\n"]}]},{"cell_type":"code","source":["## Scene Loader\n","\n","# -*- coding: utf-8 -*-\n","class THORDiscreteEnvironment(object):\n","\n","  def __init__(self, config=dict()):\n","\n","    # configurations\n","    self.scene_name          = config.get('scene_name', 'bedroom_04')\n","    self.random_start        = config.get('random_start', True)\n","    self.n_feat_per_locaiton = config.get('n_feat_per_locaiton', 1) # 1 for no sampling\n","    self.terminal_state_id   = config.get('terminal_state_id', 0)\n","\n","    self.h5_file_path = config.get('h5_file_path', 'data/%s.h5'%self.scene_name)\n","    self.h5_file      = h5py.File(self.h5_file_path, 'r')\n","\n","    self.locations   = self.h5_file['location'][()]\n","    self.rotations   = self.h5_file['rotation'][()]\n","    self.n_locations = self.locations.shape[0]\n","\n","    self.terminals = np.zeros(self.n_locations)\n","    self.terminals[self.terminal_state_id] = 1\n","    self.terminal_states, = np.where(self.terminals)\n","\n","    self.transition_graph = self.h5_file['graph'][()]\n","    self.shortest_path_distances = self.h5_file['shortest_path_distance'][()]\n","\n","    self.history_length = HISTORY_LENGTH\n","    self.screen_height  = SCREEN_HEIGHT\n","    self.screen_width   = SCREEN_WIDTH\n","\n","    # we use pre-computed fc7 features from ResNet-50\n","    # self.s_t = np.zeros([self.screen_height, self.screen_width, self.history_length])\n","    self.s_t      = np.zeros([2048, self.history_length])\n","    self.s_t1     = np.zeros_like(self.s_t)\n","    self.s_target = self._tiled_state(self.terminal_state_id)\n","\n","    self.reset()\n","\n","  # public methods\n","\n","  def reset(self):\n","    # randomize initial state\n","    while True:\n","      k = random.randrange(self.n_locations)\n","      min_d = np.inf\n","      # check if target is reachable\n","      for t_state in self.terminal_states:\n","        dist = self.shortest_path_distances[k][t_state]\n","        min_d = min(min_d, dist)\n","      # min_d = 0  if k is a terminal state\n","      # min_d = -1 if no terminal state is reachable from k\n","      if min_d > 0: break\n","\n","    # reset parameters\n","    self.current_state_id = k\n","    self.s_t = self._tiled_state(self.current_state_id)\n","\n","    self.reward   = 0\n","    self.collided = False\n","    self.terminal = False\n","\n","\n","# FOR DEBUGGING ONLY COMMENT LATER\n","  # def reset(self):\n","  #   # randomize initial state\n","  #   k = 1\n","  #   print(self.terminal_states[0])\n","  #   dist = self.shortest_path_distances[k][self.terminal_states[0]]\n","\n","  #   # reset parameters\n","  #   self.current_state_id = k\n","  #   self.s_t = self._tiled_state(self.current_state_id)\n","\n","  #   self.reward   = 0\n","  #   self.collided = False\n","  #   self.terminal = False\n","\n","  def step(self, action):\n","    assert not self.terminal, 'step() called in terminal state'\n","    k = self.current_state_id\n","    if self.transition_graph[k][action] != -1:\n","      self.current_state_id = self.transition_graph[k][action]\n","      if self.terminals[self.current_state_id]:\n","        self.terminal = True\n","        self.collided = False\n","      else:\n","        self.terminal = False\n","        self.collided = False\n","    else:\n","      self.terminal = False\n","      self.collided = True\n","\n","    self.reward = self._reward(self.terminal, self.collided)\n","    self.s_t1 = np.append(self.s_t[:,1:], self.state, axis=1)\n","\n","  def update(self):\n","    self.s_t = self.s_t1\n","\n","  # private methods\n","\n","  def _tiled_state(self, state_id):\n","    k = random.randrange(self.n_feat_per_locaiton)\n","    f = self.h5_file['resnet_feature'][state_id][k][:,np.newaxis]\n","    return np.tile(f, (1, self.history_length))\n","\n","  def _reward(self, terminal, collided):\n","    # positive reward upon task completion\n","    if terminal: return 10.0\n","    # time penalty or collision penalty\n","    return -0.1 if collided else -0.01\n","\n","  # properties\n","\n","  @property\n","  def action_size(self):\n","    # move forward/backward, turn left/right for navigation\n","    return ACTION_SIZE \n","\n","  @property\n","  def action_definitions(self):\n","    action_vocab = [\"MoveForward\", \"RotateRight\", \"RotateLeft\", \"MoveBackward\"]\n","    return action_vocab[:ACTION_SIZE]\n","\n","  @property\n","  def observation(self):\n","    return self.h5_file['observation'][self.current_state_id]\n","\n","  @property\n","  def state(self):\n","    # read from hdf5 cache\n","    k = random.randrange(self.n_feat_per_locaiton)\n","    return self.h5_file['resnet_feature'][self.current_state_id][k][:,np.newaxis]\n","\n","  @property\n","  def target(self):\n","    return self.s_target\n","\n","  @property\n","  def x(self):\n","    return self.locations[self.current_state_id][0]\n","\n","  @property\n","  def z(self):\n","    return self.locations[self.current_state_id][1]\n","\n","  @property\n","  def r(self):\n","    return self.rotations[self.current_state_id]\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J3a0P56lmyuo","executionInfo":{"status":"ok","timestamp":1651343233495,"user_tz":240,"elapsed":466,"user":{"displayName":"Bharathrushab Manthripragada","userId":"09138793677898015132"}},"outputId":"d95488b3-4714-41fd-ae4d-0d09f3cee808"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 174 ms (started: 2022-04-30 18:27:13 +00:00)\n"]}]},{"cell_type":"code","source":["## load scene into environment\n","scene_name = 'bedroom_04'\n","\n","env = THORDiscreteEnvironment({\n","'random_start': True,\n","'scene_name': scene_name,\n","'h5_file_path': 'data/%s.h5'%scene_name,\n","'terminal_state_id' : 134\n","})\n","\n","print(env.scene_name)\n","print(env.terminal_state_id)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7bCARfC4oCO9","executionInfo":{"status":"ok","timestamp":1651343266464,"user_tz":240,"elapsed":154,"user":{"displayName":"Bharathrushab Manthripragada","userId":"09138793677898015132"}},"outputId":"4626e0a1-e362-4db6-e4e2-fddae338b5f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["bedroom_04\n","134\n","time: 9.58 ms (started: 2022-04-30 18:27:46 +00:00)\n"]}]},{"cell_type":"code","source":["print(env.terminal_states[0])\n","print(env.current_state_id)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a3UwzpvYdy1K","executionInfo":{"status":"ok","timestamp":1651343270262,"user_tz":240,"elapsed":251,"user":{"displayName":"Bharathrushab Manthripragada","userId":"09138793677898015132"}},"outputId":"3a705f52-d9cc-4741-a7dd-a98cec91b77f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["134\n","302\n","time: 1.42 ms (started: 2022-04-30 18:27:49 +00:00)\n"]}]},{"cell_type":"code","source":["env.shortest_path_distances[1][134]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OWgWf3LMcXMm","executionInfo":{"status":"ok","timestamp":1651343270263,"user_tz":240,"elapsed":11,"user":{"displayName":"Bharathrushab Manthripragada","userId":"09138793677898015132"}},"outputId":"f817c5f3-1715-4893-f3a7-ee4a08284b9a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["15"]},"metadata":{},"execution_count":22},{"output_type":"stream","name":"stdout","text":["time: 3.1 ms (started: 2022-04-30 18:27:50 +00:00)\n"]}]},{"cell_type":"code","source":["c_nan = None\n","t_img_embedding = None"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zUnrJNZ-vxp_","executionInfo":{"status":"ok","timestamp":1651343270471,"user_tz":240,"elapsed":6,"user":{"displayName":"Bharathrushab Manthripragada","userId":"09138793677898015132"}},"outputId":"aec76fa6-d8e1-49d4-f10b-e65bbca6c633"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 873 µs (started: 2022-04-30 18:27:50 +00:00)\n"]}]},{"cell_type":"code","source":["class A2COriginalNetwork(nn.Module):\n","    def __init__(self):\n","        super(A2COriginalNetwork, self).__init__()\n","\n","        # Siemense layer\n","        self.fc_siemense= nn.Linear(8192, 512)\n","\n","        # Merge layer\n","        self.fc_merge = nn.Linear(1024, 512)\n","\n","\n","        # scene specific network\n","\n","        self.fc1 = nn.Linear(512, 512)\n","\n","        # Policy layer\n","        self.fc2_policy = nn.Linear(512, 4)\n","\n","        # Value layer\n","        self.fc2_value = nn.Linear(512, 1)\n","    \n","        return\n","\n","    def forward(self, inp):\n","        (x, y,) = inp\n","        \n","        x = x.view(-1)\n","        x = self.fc_siemense(x)  \n","        x = F.relu(x, True)\n","\n","        y = y.view(-1)\n","        y = self.fc_siemense(y)\n","        y = F.relu(y, True)\n","\n","        xy = torch.stack([x,y], 0).view(-1)\n","        xy = self.fc_merge(xy)\n","        xy = F.relu(xy, True)\n","\n","        # scene specific net\n","        x = self.fc1(xy)\n","        x = F.relu(x)\n","        x_policy = self.fc2_policy(x)\n","        #x_policy = F.softmax(x_policy)\n","\n","        x_value = self.fc2_value(x)[0]\n","        return (x_policy, x_value, )\n","   \n","\n","class ActorCriticLoss(nn.Module):\n","    def __init__(self, entropy_beta):\n","        self.entropy_beta = entropy_beta\n","        pass\n","\n","    def forward(self, policy, value, action_taken, temporary_difference, r):\n","        # Calculate policy entropy\n","        log_softmax_policy = torch.nn.functional.log_softmax(policy, dim=1)\n","        softmax_policy = torch.nn.functional.softmax(policy, dim=1)\n","        policy_entropy = softmax_policy * log_softmax_policy\n","        policy_entropy = -torch.sum(policy_entropy, 1)\n","\n","        # Policy loss\n","        nllLoss = F.nll_loss(log_softmax_policy, action_taken, reduce=False)\n","        policy_loss = nllLoss * temporary_difference - policy_entropy * self.entropy_beta\n","        policy_loss = policy_loss.sum(0)\n","\n","        # Value loss\n","        # learning rate for critic is half of actor's\n","        # Equivalent to 0.5 * l2 loss\n","        value_loss = (0.5 * 0.5) * F.mse_loss(value, r, size_average=False)\n","        return value_loss + policy_loss\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6yAh2zBytgFl","executionInfo":{"status":"ok","timestamp":1651343270825,"user_tz":240,"elapsed":153,"user":{"displayName":"Bharathrushab Manthripragada","userId":"09138793677898015132"}},"outputId":"4167a068-19fb-46f0-a716-da810f5e7b19"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 53.4 ms (started: 2022-04-30 18:27:50 +00:00)\n"]}]},{"cell_type":"code","source":["# from collections import namedtuple\n","\n","# TrainingSample = namedtuple('TrainingSample', ('state', 'policy', 'value', 'action_taken', 'goal', 'R', 'temporary_difference'))\n","\n","\n","#     def _forward_explore(max_local_timestep, env):\n","#         # Does the evaluation end naturally?\n","#         is_terminal = False\n","#         terminal_end = False\n","\n","#         episode_length = 0\n","#         episode_reward = 0\n","#         local_t = 0\n","\n","#         results = { \"policy\":[], \"value\": []}\n","#         rollout_path = {\"state\": [], \"action\": [], \"rewards\": [], \"done\": []}\n","\n","#         # Plays out one game to end or max_t\n","#         for t in range(max_local_timestep):\n","#             state = { \n","#                 \"current\": env.render('resnet_features'),\n","#                 \"goal\": env.render_target('resnet_features'),\n","#             }\n","\n","#             x_processed = torch.from_numpy(state[\"current\"])\n","#             goal_processed = torch.from_numpy(state[\"goal\"])\n","\n","#             (policy, value) = model(x_processed, goal_processed)\n","\n","#             # Store raw network output to use in backprop\n","#             results[\"policy\"].append(policy)\n","#             results[\"value\"].append(value)\n","\n","#             with torch.no_grad():\n","#                 (_, action,) = policy.max(0)\n","#                 action = F.softmax(policy, dim=0).multinomial(1).item()\n","            \n","#             policy = policy.data.numpy()\n","#             value = value.data.numpy()\n","            \n","            \n","#             # Makes the step in the environment\n","#             env.step(action)\n","\n","#             # Receives the game reward\n","#             is_terminal = env.is_terminal\n","\n","#             # ad-hoc reward for navigation\n","#             reward = 10.0 if is_terminal else -0.01\n","\n","#             # Max episode length\n","#             if episode_length > 5e3: is_terminal = True\n","\n","#             # Update episode stats\n","#             episode_length += 1\n","#             episode_reward += reward\n","#             # self.episode_max_q = max(self.episode_max_q, np.max(value))\n","\n","#             # clip reward\n","#             reward = np.clip(reward, -1, 1)\n","\n","#             # Increase local time\n","#             local_t += 1\n","\n","#             rollout_path[\"state\"].append(state)\n","#             rollout_path[\"action\"].append(action)\n","#             rollout_path[\"rewards\"].append(reward)\n","#             rollout_path[\"done\"].append(is_terminal)\n","\n","#             if is_terminal:\n","#                 # TODO: add logging\n","#                 print('playout finished')\n","#                 print(f'episode length: {self.episode_length}')\n","#                 print(f'episode reward: {self.episode_reward}')\n","#                 print(f'episode max_q: {self.episode_max_q}')\n","\n","#                 terminal_end = True\n","#                 self._reset_episode()\n","#                 break\n","\n","#         if terminal_end:\n","#             return 0.0, results, rollout_path\n","#         else:\n","#             x_processed = torch.from_numpy(env.render('resnet_features'))\n","#             goal_processed = torch.from_numpy(env.render_target('resnet_features'))\n","\n","#             (_, value) = self.policy_network((x_processed, goal_processed,))\n","#             return value.data.item(), results, rollout_path\n","    \n","#     def _optimize_path(self, playout_reward: float, results, rollout_path, gamma, loss_criterion, optimizer):\n","#         policy_batch = []\n","#         value_batch = []\n","#         action_batch = []\n","#         temporary_difference_batch = []\n","#         playout_reward_batch = []\n","\n","\n","#         for i in reversed(range(len(results[\"value\"]))):\n","#             reward = rollout_path[\"rewards\"][i]\n","#             value = results[\"value\"][i]\n","#             action = rollout_path[\"action\"][i]\n","\n","#             playout_reward = reward + gamma * playout_reward\n","#             temporary_difference = playout_reward - value.data.item()\n","\n","#             policy_batch.append(results['policy'][i])\n","#             value_batch.append(results['value'][i])\n","#             action_batch.append(action)\n","#             temporary_difference_batch.append(temporary_difference)\n","#             playout_reward_batch.append(playout_reward)\n","        \n","#         policy_batch = torch.stack(policy_batch, 0)\n","#         value_batch = torch.stack(value_batch, 0)\n","#         action_batch = torch.from_numpy(np.array(action_batch, dtype=np.int64))\n","#         temporary_difference_batch = torch.from_numpy(np.array(temporary_difference_batch, dtype=np.float32))\n","#         playout_reward_batch = torch.from_numpy(np.array(playout_reward_batch, dtype=np.float32))\n","        \n","#         # Compute loss\n","#         loss = loss_criterion.forward(policy_batch, value_batch, action_batch, temporary_difference_batch, playout_reward_batch)\n","#         loss = loss.sum()\n","\n","#         # loss_value = loss.detach().numpy()\n","#         optimizer.zero_grad()\n","#         loss.backward()\n","#         optimizer.step()\n","\n","#         return\n","\n","#     def run(self, master = None):\n","       \n","#             self.env.reset()\n","#             while True:\n","#                 self._sync_network()\n","#                 # Plays some samples\n","#                 playout_reward, results, rollout_path = self._forward_explore()\n","#                 # Train on collected samples\n","#                 self._optimize_path(playout_reward, results, rollout_path)\n","                \n","#                 print(f'Step finished {self.optimizer.get_global_step()}')\n","\n","#                 # Trigger save or other\n","#                 self.saver.after_optimization()                \n","#                 pass\n","#         except Exception as e:\n","#             print(e)\n","#             # TODO: add logging\n","#             #self.logger.error(e.msg)\n","#             raise e"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p9pWlBVLOj-s","executionInfo":{"status":"ok","timestamp":1651343271071,"user_tz":240,"elapsed":20,"user":{"displayName":"Bharathrushab Manthripragada","userId":"09138793677898015132"}},"outputId":"eaf3c523-392f-41df-e6cd-790abf81ac83"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 16.3 ms (started: 2022-04-30 18:27:50 +00:00)\n"]}]},{"cell_type":"code","source":["num_epochs = 2000\n","model = A2COriginalNetwork()\n","\n","lr = 1e-2\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","device_name = 'cuda:0'\n","# device_name = 'cpu'\n","\n","max_local_timestep = 5\n","entropy_beta = 1e-3\n","\n","torch.autograd.set_detect_anomaly(True)\n","\n","loss_model = ActorCriticLoss(entropy_beta)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"18ySNjFJ0inq","executionInfo":{"status":"ok","timestamp":1651343271226,"user_tz":240,"elapsed":6,"user":{"displayName":"Bharathrushab Manthripragada","userId":"09138793677898015132"}},"outputId":"9bd60ba9-bfbb-4ce5-c05a-1b933dab72a0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 48 ms (started: 2022-04-30 18:27:51 +00:00)\n"]}]},{"cell_type":"code","source":["from collections import namedtuple\n","\n","TrainingSample = namedtuple('TrainingSample', ('state', 'policy', 'value', 'action_taken', 'goal', 'R', 'temporary_difference'))\n","\n","model.to(device_name)\n","\n","for epoch in tqdm(range(num_epochs)):\n","\n","    epoch_loss = 0\n","    # model.train()\n","\n","    env.reset()\n","\n","    terminal_end= False\n","\n","    episode_length = 0\n","    episode_reward = 0\n","    local_t = 0\n","\n","    results = { \"policy\":[], \"value\": []}\n","    rollout_path = {\"state\": [], \"action\": [], \"rewards\": [], \"done\": []}\n","\n","\n","    # Plays out one game to end or max_t\n","    for t in range(max_local_timestep):\n","\n","        torch_s_t = env.s_t  # state embedding 2048 x 4\n","        torch_target = env.target  # target embedding \n","\n","        state = {\"current\": torch_s_t,\n","                  \"goal\": torch_target,\n","                }\n","\n","        x_processed = torch.from_numpy(state[\"current\"])\n","        goal_processed = torch.from_numpy(state[\"goal\"])\n","\n","        (policy, value) = model((x_processed.to(device_name), goal_processed.to(device_name)))\n","\n","        # Store raw network output to use in backprop\n","        results[\"policy\"].append(policy)\n","        results[\"value\"].append(value)\n","\n","        with torch.no_grad():\n","            (_, action,) = policy.max(0)\n","            action = F.softmax(policy, dim=0).multinomial(1).item()\n","        \n","        policy = policy.data.cpu().numpy()\n","        value = value.data.cpu().numpy()\n","        \n","        \n","        # Makes the step in the environment\n","        env.step(action)\n","\n","        # Receives the game reward\n","        is_terminal = env.terminal\n","\n","        # ad-hoc reward for navigation\n","        reward = 10.0 if is_terminal else -0.01\n","\n","        # Max episode length\n","        if episode_length > 5e3: is_terminal = True\n","\n","        # Update episode stats\n","        episode_length += 1\n","        episode_reward += reward\n","        # self.episode_max_q = max(self.episode_max_q, np.max(value))\n","\n","        # clip reward\n","        reward = np.clip(reward, -1, 1)\n","\n","        # Increase local time\n","        local_t += 1\n","\n","        rollout_path[\"state\"].append(state)\n","        rollout_path[\"action\"].append(action)\n","        rollout_path[\"rewards\"].append(reward)\n","        rollout_path[\"done\"].append(is_terminal)\n","\n","        if is_terminal:\n","            # TODO: add logging\n","            print('playout finished')\n","            print(f'episode length: {episode_length}')\n","            print(f'episode reward: {episode_reward}')\n","            # print(f'episode max_q: {episode_max_q}')\n","\n","            terminal_end = True\n","            break\n","\n","    if terminal_end:\n","        # playout_reward, results, rollout_path =  0.0, results, rollout_path\n","        playout_reward =  0.0\n","\n","    else:\n","\n","        torch_s_t = env.s_t  # state embedding 2048 x 4\n","        torch_target = env.target  # target embedding \n","\n","        state = {\"current\": torch_s_t,\n","                  \"goal\": torch_target,\n","                }\n","\n","        x_processed = torch.from_numpy(state[\"current\"])\n","        goal_processed = torch.from_numpy(state[\"goal\"])\n","\n","        (_, value) = model((x_processed.to(device_name), goal_processed.to(device_name)))\n","\n","        # playout_reward, results, rollout_path = value.data.item(), results, rollout_path\n","        playout_reward = value.data.item()\n","\n","\n","    policy_batch = []\n","    value_batch = []\n","    action_batch = []\n","    temporary_difference_batch = []\n","    playout_reward_batch = []\n","\n","\n","    for i in reversed(range(len(results[\"value\"]))):\n","        reward = rollout_path[\"rewards\"][i]\n","        value = results[\"value\"][i]\n","        action = rollout_path[\"action\"][i]\n","\n","        playout_reward = reward + GAMMA * playout_reward\n","        temporary_difference = playout_reward - value.data.item()\n","\n","        policy_batch.append(results['policy'][i])\n","        value_batch.append(results['value'][i])\n","        action_batch.append(action)\n","        temporary_difference_batch.append(temporary_difference)\n","        playout_reward_batch.append(playout_reward)\n","    \n","    policy_batch = torch.stack(policy_batch, 0)\n","    value_batch = torch.stack(value_batch, 0)\n","    action_batch = torch.from_numpy(np.array(action_batch, dtype=np.int64)).to(device_name)\n","    temporary_difference_batch = torch.from_numpy(np.array(temporary_difference_batch, dtype=np.float32)).to(device_name)\n","    playout_reward_batch = torch.from_numpy(np.array(playout_reward_batch, dtype=np.float32)).to(device_name)\n","    \n","    # Compute loss\n","    loss = loss_model.forward(policy_batch, value_batch, action_batch, temporary_difference_batch, playout_reward_batch)\n","    loss = loss.sum()\n","\n","    # print(loss)\n","\n","    # loss_value = loss.detach().numpy()\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U7MXIRJNO8hC","executionInfo":{"status":"ok","timestamp":1651343510014,"user_tz":240,"elapsed":194014,"user":{"displayName":"Bharathrushab Manthripragada","userId":"09138793677898015132"}},"outputId":"ee8ff543-9a26-4fc9-818b-34c81d03c955"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/2000 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n","  1%|          | 13/2000 [00:01<02:48, 11.81it/s]"]},{"output_type":"stream","name":"stdout","text":["playout finished\n","episode length: 2\n","episode reward: 9.99\n"]},{"output_type":"stream","name":"stderr","text":["  2%|▏         | 49/2000 [00:04<02:43, 11.95it/s]"]},{"output_type":"stream","name":"stdout","text":["playout finished\n","episode length: 1\n","episode reward: 10.0\n"]},{"output_type":"stream","name":"stderr","text":["  5%|▌         | 103/2000 [00:09<02:42, 11.70it/s]"]},{"output_type":"stream","name":"stdout","text":["playout finished\n","episode length: 1\n","episode reward: 10.0\n"]},{"output_type":"stream","name":"stderr","text":["  9%|▉         | 177/2000 [00:16<02:42, 11.22it/s]"]},{"output_type":"stream","name":"stdout","text":["playout finished\n","episode length: 3\n","episode reward: 9.98\n"]},{"output_type":"stream","name":"stderr","text":[" 10%|▉         | 199/2000 [00:18<02:30, 11.93it/s]"]},{"output_type":"stream","name":"stdout","text":["playout finished\n","episode length: 1\n","episode reward: 10.0\n"]},{"output_type":"stream","name":"stderr","text":[" 10%|█         | 207/2000 [00:19<02:27, 12.17it/s]"]},{"output_type":"stream","name":"stdout","text":["playout finished\n","episode length: 1\n","episode reward: 10.0\n"]},{"output_type":"stream","name":"stderr","text":[" 13%|█▎        | 257/2000 [00:24<02:34, 11.26it/s]"]},{"output_type":"stream","name":"stdout","text":["playout finished\n","episode length: 3\n","episode reward: 9.98\n"]},{"output_type":"stream","name":"stderr","text":[" 20%|█▉        | 395/2000 [00:37<02:27, 10.92it/s]"]},{"output_type":"stream","name":"stdout","text":["playout finished\n","episode length: 1\n","episode reward: 10.0\n"]},{"output_type":"stream","name":"stderr","text":[" 21%|██▏       | 429/2000 [00:41<02:19, 11.24it/s]"]},{"output_type":"stream","name":"stdout","text":["playout finished\n","episode length: 2\n","episode reward: 9.99\n"]},{"output_type":"stream","name":"stderr","text":[" 22%|██▏       | 437/2000 [00:41<02:12, 11.82it/s]"]},{"output_type":"stream","name":"stdout","text":["playout finished\n","episode length: 2\n","episode reward: 9.99\n"]},{"output_type":"stream","name":"stderr","text":[" 27%|██▋       | 541/2000 [00:51<02:09, 11.29it/s]"]},{"output_type":"stream","name":"stdout","text":["playout finished\n","episode length: 2\n","episode reward: 9.99\n"]},{"output_type":"stream","name":"stderr","text":[" 28%|██▊       | 565/2000 [00:54<02:06, 11.39it/s]"]},{"output_type":"stream","name":"stdout","text":["playout finished\n","episode length: 2\n","episode reward: 9.99\n"]},{"output_type":"stream","name":"stderr","text":[" 30%|██▉       | 598/2000 [00:57<02:04, 11.30it/s]"]},{"output_type":"stream","name":"stdout","text":["playout finished\n","episode length: 3\n","episode reward: 9.98\n"]},{"output_type":"stream","name":"stderr","text":[" 34%|███▎      | 674/2000 [01:04<01:58, 11.23it/s]"]},{"output_type":"stream","name":"stdout","text":["playout finished\n","episode length: 2\n","episode reward: 9.99\n"]},{"output_type":"stream","name":"stderr","text":[" 41%|████      | 824/2000 [01:19<01:43, 11.36it/s]"]},{"output_type":"stream","name":"stdout","text":["playout finished\n","episode length: 1\n","episode reward: 10.0\n"]},{"output_type":"stream","name":"stderr","text":[" 45%|████▌     | 902/2000 [01:26<01:42, 10.70it/s]"]},{"output_type":"stream","name":"stdout","text":["playout finished\n","episode length: 5\n","episode reward: 9.96\n"]},{"output_type":"stream","name":"stderr","text":[" 46%|████▌     | 918/2000 [01:28<01:30, 11.90it/s]"]},{"output_type":"stream","name":"stdout","text":["playout finished\n","episode length: 1\n","episode reward: 10.0\n"]},{"output_type":"stream","name":"stderr","text":[" 58%|█████▊    | 1152/2000 [01:50<01:16, 11.04it/s]"]},{"output_type":"stream","name":"stdout","text":["playout finished\n","episode length: 4\n","episode reward: 9.97\n"]},{"output_type":"stream","name":"stderr","text":[" 58%|█████▊    | 1156/2000 [01:51<01:16, 11.03it/s]"]},{"output_type":"stream","name":"stdout","text":["playout finished\n","episode length: 5\n","episode reward: 9.96\n"]},{"output_type":"stream","name":"stderr","text":[" 89%|████████▉ | 1789/2000 [02:53<00:18, 11.12it/s]"]},{"output_type":"stream","name":"stdout","text":["playout finished\n","episode length: 4\n","episode reward: 9.97\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 2000/2000 [03:13<00:00, 10.33it/s]"]},{"output_type":"stream","name":"stdout","text":["time: 3min 13s (started: 2022-04-30 18:28:36 +00:00)\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["torch.save(model, 'orig_torch_model_2000_epochs.pth')"],"metadata":{"id":"tSx1RJdMK6Xx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651304350161,"user_tz":240,"elapsed":440,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}},"outputId":"a7ba8a14-429d-4c90-ba57-2b5504ed730f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 63.1 ms (started: 2022-04-30 07:39:08 +00:00)\n"]}]},{"cell_type":"code","source":["!cp -r 'orig_torch_model_2000_epochs.pth' '/content/gdrive/MyDrive/ESE_650_Project/models/'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t8bc2r5XJHly","executionInfo":{"status":"ok","timestamp":1651304362695,"user_tz":240,"elapsed":380,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}},"outputId":"080ef5fc-81bf-4cd4-9a14-a8abc3ee946e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 156 ms (started: 2022-04-30 07:39:21 +00:00)\n"]}]},{"cell_type":"markdown","source":["#Cells not used"],"metadata":{"id":"Rga9FoeZbZgL"}},{"cell_type":"code","source":["# model.to(device_name)\n","\n","# for epoch in tqdm(range(num_epochs)):\n","\n","#     epoch_loss = 0\n","#     # model.double()\n","#     model.train()\n","#     # take 4 random actions intially for warm start\n","\n","#     env.reset()\n","\n","#     states = []\n","#     actions = []\n","#     policy = []\n","#     rewards = []\n","#     values = []\n","#     targets = []\n","\n","#     terminal_reached = False\n","\n","#     episode_length = 0\n","#     episode_reward = 0\n","#     local_t = 0\n","\n","#     for t in range(max_local_timestep):\n","\n","#         torch_s_t = torch.from_numpy(\n","#             env.s_t).float().flatten()  # state embedding in torch convertinng from 2048 x 4, to 8192 x 1\n","#         torch_target = torch.from_numpy(env.target).float().flatten()  # target embedding in torch\n","\n","#         torch_s_t = torch_s_t.to(device_name)\n","#         torch_target = torch_target.to(device_name)\n","\n","#         actions_prob, value = model(torch_s_t, torch_target)\n","\n","#         action = torch.multinomial(actions_prob,\n","#                                    1)  # sample action according to the probability returned by the network\n","\n","#         # store required items\n","#         states.append(env.s_t)\n","#         policy.append(actions_prob)\n","#         actions.append(action)\n","#         values.append(value)\n","#         targets.append(env.target)\n","\n","#         # process game\n","#         env.step(action)\n","\n","#         # receive game result\n","#         reward = env.reward\n","#         terminal = env.terminal\n","\n","#         # ad-hoc reward for navigation\n","#         reward = 10.0 if terminal else -0.01\n","#         if episode_length > 5e3: terminal = True\n","\n","#         episode_reward += reward\n","#         episode_length += 1\n","#         # episode_max_q = max(episode_max_q, np.max(value_)) !!!! DO Q VALUE CLIPPING LATER !!!!!!!\n","\n","#         # clip and append reward\n","#         rewards.append(np.clip(reward, -1, 1))\n","\n","#         local_t += 1\n","\n","#         # s_t1 -> s_t\n","#         env.update()\n","\n","#         if terminal:\n","#             terminal_end = True\n","#             # sys.stdout.write(\"time %d | thread #%d | scene %s | target #%s\\n%s %s episode reward = %.3f\\n%s %s episode length = %d\\n%s %s episode max Q  = %.3f\\n\" % (global_t, self.thread_index, self.scene_scope, self.task_scope, self.scene_scope, self.task_scope, self.episode_reward, self.scene_scope, self.task_scope, self.episode_length, self.scene_scope, self.task_scope, self.episode_max_q))\n","\n","#             # summary_values = {\n","#             # \"episode_reward_input\": episode_reward,\n","#             # \"episode_length_input\": float(episode_length),\n","#             # # \"episode_max_q_input\": episode_max_q,\n","#             # # \"learning_rate_input\": _anneal_learning_rate(global_t)\n","#             # }\n","\n","#             # self._record_score(sess, summary_writer, summary_op, summary_placeholders,\n","#             #                 summary_values, global_t)\n","#             # self.episode_reward = 0\n","#             # self.episode_length = 0\n","#             # self.episode_max_q = -np.inf\n","#             # self.env.reset()\n","\n","#             break\n","\n","#     torch_s_t = torch.from_numpy(env.s_t).float().flatten()  # state embedding in torch\n","#     torch_target = torch.from_numpy(env.target).float().flatten()  # target embedding in torch\n","\n","#     torch_s_t = torch_s_t.to(device_name)\n","#     torch_target = torch_target.to(device_name)\n","\n","#     next_state_value = 0.0\n","#     if not terminal:\n","#         _, next_state_value = model(torch_s_t, torch_target)\n","#         next_state_value = next_state_value.item()\n","\n","#     actions.reverse()\n","#     states.reverse()\n","#     rewards.reverse()\n","#     values.reverse()\n","#     policy.reverse()\n","\n","#     batch_si = []\n","#     batch_policy = []\n","#     batch_a = []\n","#     batch_td = []\n","#     batch_R = []\n","#     batch_t = []\n","#     batch_values = []\n","\n","#     # compute and accmulate gradients\n","#     for (pi, ai, ri, si, Vi, ti) in zip(policy, actions, rewards, states, values, targets):\n","#         next_state_value = ri + GAMMA * next_state_value\n","#         td = next_state_value - Vi\n","#         a = np.zeros([ACTION_SIZE])\n","#         a[ai] = 1\n","\n","#         batch_si.append(si)\n","#         batch_policy.append(pi)\n","#         batch_a.append(a)\n","#         batch_td.append(td)\n","#         batch_R.append(next_state_value)\n","#         batch_t.append(ti)\n","#         batch_values.append(Vi)\n","\n","#     # COMPUTE LOSS + BACKPROP\n","#     # !!!!!!!! compute actor/policy loss !!!!!!!!!!!!\n","#     batch_policy_tensor = torch.stack(batch_policy, 0)\n","\n","#     batch_policy_tensor = torch.clip(batch_policy_tensor, 1e-15, 1.0)\n","\n","#     policy_log_prob = torch.log(batch_policy_tensor)\n","#     # policy_entropy = - torch.sum(torch.mean(batch_policy_tensor, axis=1) * policy_log_prob, axis=1)\n","#     policy_entropy = - torch.sum(batch_policy_tensor * policy_log_prob, axis=1)\n","\n","#     # batch_a_tensor = torch.from_numpy(np.stack(batch_a, axis=0)).float()\n","#     batch_a_tensor = torch.from_numpy(np.array(batch_a, dtype=np.int64)).to(device_name)\n","#     # print('llll',-torch.sum(policy_log_prob * batch_a_tensor, axis=1))\n","#     # print('jdkji',policy_log_prob * batch_a_tensor)\n","#     # nllLoss = F.nll_loss(policy_log_prob, batch_a_tensor, reduce=False)\n","#     # nllLoss = -torch.sum(policy_log_prob * batch_a_tensor, axis=1)\n","#     nllLoss = F.nll_loss(policy_log_prob, torch.argmax(batch_a_tensor, dim=1), reduce=False)\n","\n","#     batch_td_tensor = torch.stack(batch_td)\n","#     policy_loss = nllLoss * batch_td_tensor - policy_entropy * entropy_beta\n","#     policy_loss = policy_loss.sum()\n","\n","#     # !!!!!!!!!!!! Value loss !!!!!!!!!!!!!!\n","#     # learning rate for critic is half of actor's\n","#     # Equivalent to 0.5 * l2 loss\n","#     # value_loss = (0.5 * 0.5) * F.mse_loss(batch_td_tensor, torch.zeros_like(batch_td_tensor))\n","\n","#     r_tensor = torch.from_numpy(np.array(batch_R, dtype=np.float32)).reshape(-1,1).to(device_name)\n","#     value_loss = (0.5 * 0.5) * F.mse_loss(torch.stack(batch_values), r_tensor)\n","#     total_loss = value_loss + policy_loss\n","\n","#     # if torch.isnan(total_loss).any():\n","#     #     print('\\n\\n\\ntotal loss contains nans\\n\\n\\n')\n","#     #     break\n","\n","#     optimizer.zero_grad()\n","#     total_loss.backward()\n","#     optimizer.step()"],"metadata":{"id":"FuMFQzTSxBnd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651304333995,"user_tz":240,"elapsed":36,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}},"outputId":"b363a745-5e03-446e-9b6c-91af244955bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 21.3 ms (started: 2022-04-30 07:38:52 +00:00)\n"]}]},{"cell_type":"code","source":["#@title\n","# model = torch.load('model.pth')"],"metadata":{"id":"bcOCBbuZMBqR","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title\n","# policy_log_prob"],"metadata":{"id":"yjc_u6l90ORh","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print(c_nan)\n","# print(torch.isnan(t_img_embedding).any())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":209},"id":"RKwzD6sjwGBm","executionInfo":{"status":"error","timestamp":1650338066064,"user_tz":240,"elapsed":105,"user":{"displayName":"Vasanth Kolli","userId":"04609767155968044546"}},"outputId":"ad0fb894-cc5e-48e2-a453-1eb943e3d888"},"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-126-7cd5a46ff3a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# print(c_nan)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_img_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: isnan(): argument 'input' (position 1) must be Tensor, not NoneType"]},{"output_type":"stream","name":"stdout","text":["time: 15.1 ms (started: 2022-04-19 03:14:25 +00:00)\n"]}]},{"cell_type":"code","source":["# for i, k in enumerate(model.parameters()):\n","#     if i == 2:\n","#         print(k[(torch.logical_not(torch.isnan(k)))].min())\n","#         print()"],"metadata":{"id":"aGp16sWWxdEh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # save the model parameters\n","# import time\n","\n","# timestr = time.strftime(\"%d_%m-%H_%M_%S\")\n","# save_model_filename = env.scene_name +'ActorCritic_model_weights_epoch_' + str(num_epochs) + '_' + timestr  + '.pth'\n","# torch.save(model, save_model_filename)"],"metadata":{"id":"k26Pnvf_roEP","executionInfo":{"status":"ok","timestamp":1651286287079,"user_tz":240,"elapsed":178,"user":{"displayName":"Bharathrushab Manthripragada","userId":"09138793677898015132"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7aede593-4f65-4244-a57d-1683b9240d7b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 46.7 ms (started: 2022-04-30 02:38:06 +00:00)\n"]}]},{"cell_type":"code","source":["# !cp {save_model_filename} '/content/gdrive/MyDrive//ESE_650_Project/models/'"],"metadata":{"id":"AchqiO1tfV5d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651286290240,"user_tz":240,"elapsed":195,"user":{"displayName":"Bharathrushab Manthripragada","userId":"09138793677898015132"}},"outputId":"23879414-1d00-4c17-9d03-12e83de251db"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 159 ms (started: 2022-04-30 02:38:09 +00:00)\n"]}]},{"cell_type":"code","source":["\n","\n","# # from https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f\n","\n","# for episode in range(max_episodes):\n","#         log_probs = []\n","#         values = []\n","#         rewards = []\n","\n","#         state = env.reset()\n","#         for steps in range(num_steps):\n","#             value, policy_dist = actor_critic.forward(state)\n","#             value = value.detach().numpy()[0,0]\n","#             dist = policy_dist.detach().numpy() \n","\n","#             action = np.random.choice(num_outputs, p=np.squeeze(dist))\n","#             log_prob = torch.log(policy_dist.squeeze(0)[action])\n","#             entropy = -np.sum(np.mean(dist) * np.log(dist))\n","#             new_state, reward, done, _ = env.step(action)\n","\n","#             rewards.append(reward)\n","#             values.append(value)\n","#             log_probs.append(log_prob)\n","#             entropy_term += entropy\n","#             state = new_state\n","            \n","#             if done or steps == num_steps-1:\n","#                 Qval, _ = actor_critic.forward(new_state)\n","#                 Qval = Qval.detach().numpy()[0,0]\n","#                 all_rewards.append(np.sum(rewards))\n","#                 all_lengths.append(steps)\n","#                 average_lengths.append(np.mean(all_lengths[-10:]))\n","#                 if episode % 10 == 0:                    \n","#                     sys.stdout.write(\"episode: {}, reward: {}, total length: {}, average length: {} \\n\".format(episode, np.sum(rewards), steps, average_lengths[-1]))\n","#                 break\n","        \n","#         # compute Q values\n","#         Qvals = np.zeros_like(values)\n","#         for t in reversed(range(len(rewards))):\n","#             Qval = rewards[t] + GAMMA * Qval\n","#             Qvals[t] = Qval\n","  \n","#         #update actor critic\n","#         values = torch.FloatTensor(values)\n","#         Qvals = torch.FloatTensor(Qvals)\n","#         log_probs = torch.stack(log_probs)\n","        \n","#         advantage = Qvals - values\n","#         actor_loss = (-log_probs * advantage).mean()\n","#         critic_loss = 0.5 * advantage.pow(2).mean()\n","#         ac_loss = actor_loss + critic_loss + 0.001 * entropy_term\n","\n","#         ac_optimizer.zero_grad()\n","#         ac_loss.backward()\n","#         ac_optimizer.step()"],"metadata":{"id":"0RqMyKS-8h5q","executionInfo":{"status":"ok","timestamp":1650311410041,"user_tz":240,"elapsed":5,"user":{"displayName":"Bharathrushab Manthripragada","userId":"13653408937867628569"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"772bb29d-1cc3-400b-989d-f2bf55bf3de7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 12.4 ms (started: 2022-04-18 19:50:09 +00:00)\n"]}]},{"cell_type":"markdown","source":["# Evaluation"],"metadata":{"id":"EWXnJW9j182X"}},{"cell_type":"code","source":["\n","# for episode in tqdm(range(NUM_EVAL_EPISODES)):\n","for episode in tqdm(range(1)):\n","\n","\n","    model.eval()\n","    env.reset()\n","\n","    actions = []\n","\n","    final_state = None\n","    inital_state_id = env.current_state_id\n","\n","    terminal_reached = False\n","\n","    episode_length = 0\n","    episode_reward = 0\n","    local_t = 0\n","\n","    terminal = False\n","    inital_state_to_target_dist = env.shortest_path_distances[inital_state_id][env.terminal_state_id]\n","    \n","\n","    while not terminal:\n","\n","        torch_s_t = torch.from_numpy(env.s_t).float().flatten()  # state embedding in torch convertinng from 2048 x 4, to 8192 x 1\n","        torch_target = torch.from_numpy(env.target).float().flatten()  # target embedding in torch\n","\n","        torch_s_t = torch_s_t.to(device_name)\n","        torch_target = torch_target.to(device_name)\n","\n","        actions_prob, value = model((torch_s_t, torch_target))\n","\n","        action = torch.argmax(actions_prob)  # choose best action\n","\n","        # store required items\n","        actions.append(action)\n","\n","        # process game\n","        env.step(action)\n","        print(env.current_state_id)\n","\n","        # receive game result\n","        reward = env.reward\n","        terminal = env.terminal\n","\n","        # ad-hoc reward for navigation\n","        reward = 10.0 if terminal else -0.01\n","        # if episode_length > 5e3: terminal = True\n","        if episode_length > 50: terminal = True\n","\n","\n","        episode_reward += reward\n","        episode_length += 1\n","        # episode_max_q = max(episode_max_q, np.max(value_)) !!!! DO Q VALUE CLIPPING LATER !!!!!!!\n","\n","        local_t += 1\n","\n","        # s_t1 -> s_t\n","        env.update()\n","\n","        if terminal:\n","            final_state = env.s_t[0]\n","            terminal_end = True\n","            break\n","\n","    \n","    final_state_to_target_dist = env.shortest_path_distances[env.current_state_id][env.terminal_state_id]\n","    print(f'\\nnum_iter : {episode}\\nepisode_length : {episode_length}')\n","    print(f'inital state to target state shortest dist: {inital_state_to_target_dist}')\n","    # print(f'shortest dist from episode termination state to goal state: {dist}')\n","    print(f'final state to target state shortest dist: {final_state_to_target_dist}')\n","    print()\n","\n","    "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ldOqxD5X2HDO","executionInfo":{"status":"ok","timestamp":1651343981953,"user_tz":240,"elapsed":957,"user":{"displayName":"Bharathrushab Manthripragada","userId":"09138793677898015132"}},"outputId":"4db95c70-3605-49d9-f0ba-6f09a424a4a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/1 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["372\n","373\n","374\n","375\n","372\n","373\n","374\n","375\n","372\n","373\n","374\n","375\n","372\n","373\n","374\n","375\n","372\n","373\n","374\n","375\n","372\n","373\n","374\n","375\n","372\n","373\n","374\n","375\n","372\n","373\n","374\n","375\n","372\n","373\n","374\n","375\n","372\n","373\n","374\n","375\n","372\n","373\n","374\n","375\n","372\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  1.40it/s]"]},{"output_type":"stream","name":"stdout","text":["373\n","374\n","375\n","372\n","373\n","374\n","375\n","\n","num_iter : 0\n","episode_length : 52\n","inital state to target state shortest dist: 9\n","final state to target state shortest dist: 9\n","\n","time: 765 ms (started: 2022-04-30 18:39:40 +00:00)\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"5BUT46P_vp4q"},"execution_count":null,"outputs":[]}]}